\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{bbm}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[style=authoryear,sorting=nyt]{biblatex}
\newtheorem{lemma}{Lemma}
\addbibresource{ref.bib}
\setlength{\parindent}{0cm}
\setlength{\parskip}{1em}

\newenvironment{boenumerate}
    {\begin{enumerate}\renewcommand\labelenumi{\textbf\theenumi}}
    {\end{enumerate}}
\numberwithin{equation}{section}
\begin{document}
\title{Notes for Kalman Filter EM algorithms}

\section{Introduction:}
\texttt{linkalman} is a python package that solves linear structural time series models with Gaussian noises. Compared with some other popular Kalman filter packages written in python, \texttt{linkalman} has several advantages:
\begin{boenumerate}
    \item Account for incomplete measurements 
    \item Flexible model structure
    \item Robust and efficient implementation
    \item Exact initialization
\end{boenumerate}
Kalman filtering is a technique that provides an elegant solution to a wide range of time series problems. When I started learning Kalman filtering, I found most existing Kalman filter packages and code snippets implement algorithms based on simple textbook models that are over-simplified for pedagogical purpose. In practice, a dynamic system may assume complex functional forms and may have incomplete measurements. In addition, solving a Kalman filter requires estimates on initial conditions, which is rarely satisfied in real world problems. Finally, numerical implementation of Kalman filter algorithms are vulnerable to failures from rounding errors. \texttt{linkalman} package provides a solid solution to all these challenges. 

This document is a product of my learning on Kalman filter and serves as the theoretical foundation and user manual of \texttt{linkalman}. Section (\ref{sec:model_setup}) lays out the general structure of a Hidden Markov Model (HMM). Section (\ref{sec:filter}) and (\ref{sec:smoother}) provide detailed discussions on Kalman filters and Kalman smoothers, respectively. Section (\ref{sec:EM}) presents the EM algorithm to estimate underlying parameters of a HMM, Section (\ref{sec:codebase}) explains the package design of \texttt{linkalman}, and Section (\ref{sec:apply}) presents applications of Kalman filtering. 

\section{Model Setup:} \label{sec:model_setup}
Consider the following Linear Dynamic System:
\begin{align}
    \xi_{t+1} = & F_{t}\xi_{t} + B_{t}x_t + v_t \label{eq:state_evolve} \\
    y_t = & H_t\xi_{t} + D_{t}x_t + w_t \label{eq:measure}
\end{align}
Equation (\ref{eq:state_evolve}) governs a Markov state transition process. $\xi_t$ $(m\times 1)$ is the latent state random vector at time $t$, $x_t$ $(k\times 1)$ is the deterministic input signal, $v_t$ $(m\times 1)$ is the exogenous process noise. We assume that $v_t\sim \mathcal{N}(0,Q_t)$ is white noise\footnote{For process noises that are not white noise, we can re-write equation (\ref{eq:state_evolve}) to maintain independence across time. See Section (\ref{sec:apply}) for examples}. $F_t$ $(m\times m)$, $B_t$ $(m\times k)$, and $v_{t+1}$ specify the transition dynamics between $t$ and $t+1$. 

Equation (\ref{eq:measure}) is the measurement specification. $y_t$ $(n\times 1)$ is the measurement vector at time $t$. $w_t$ $(n\times 1)$ is the exogenous measurement noise. In addition to assuming $w_t\sim \mathcal{N}(0, R_t)$ is white noise, I also assume that  $w_t \perp v_s \ \forall\  t,s\in\{0,1,...,T\}$. $H_t$ $(n\times m)$ and $D_t$ $(n\times k)$ dictate interaction among $\xi_t$, $y_t$ and $x_t$. 

Equations (\ref{eq:state_evolve}) and (\ref{eq:measure}) characterize an HMM, with system matrices $M_t$ defined as:
\begin{align*}
M_t\equiv\{F_t, B_t, H_t, D_t, Q_t, R_t\}
\end{align*}
The subscript $t$ allows flexible model specification. For example, regression effects in time series models are placed in $B_t x_t$; an ARMA process can be modeled by $F_t\xi_t$ and $v_t$; additive outliers fit into $B_t x_t$. We bind $M_t$ with some underlying parameter set $\theta$. For example cyclical pattern can be modeled by using Trigonometric Cycles (\cite{harvey_1985}).  

\section{Kalman Filter} \label{sec:filter}
\subsection{Filtering with Known Initial Conditions:}

Given a set of measurements over time, and suppose $\theta$ (i.e. $M_t$) is known. To predict $y_{T+1}$, we need to perform forward filtering. We start by making the following notations:
\begin{align*}
    Y_t &\equiv \{y_1, y_2, ..., y_t\} \\
    X_t &\equiv \{x_1, x_2, ..., x_t\} \\
    \Xi_t &\equiv \{\xi_1,\xi_2,...,\xi_t\} \\
    \hat{\xi}_{t,s} &\equiv E(\xi_t|X_{s},Y_{s};\theta) \\
    \hat{\xi}_{t,r,s} &\equiv E(\xi_t|\xi_r,X_{s},Y_{s};\theta) \\
    P_{t,s} &\equiv E[(\xi_t-\hat{\xi}_{t,s})(\xi_t-\hat{\xi}_{t,s})']
\end{align*}
With the assumption that $w_t$ and $v_t$ are Gaussian white noises, conditional distribution of $y_t$ and $\xi_t$ are fully characterized by Gaussian process with $\hat{\xi}_{t,s}$ and $P_{t,s}$. Now consider at time $t$, we know $Y_t$ and $M_t$, and suppose we also know $\hat{\xi}_{t,t-1}$ and $P_{t,t-1}$. By equation (\ref{eq:state_evolve}), we have\footnote{The derivation closely follows Chapter 13 in (\cite{hamilton_1994}). In particular, derivation of equation (\ref{eq:p_tt}) is given in Appendix \ref{ap:iter_proj}.}:
\begin{align}
    \hat{\xi}_{t,t} &= \hat{\xi}_{t,t-1} + K_t(y_t-H_t\hat{\xi}_{t,t-1}-D_tx_t) \label{eq:filter_begin} \\
    K_t &= P_{t,t-1}H_t^{'}(H_tP_{t,t-1}H_t^{'}+R_t)^{-1} \label{eq:gain} \\
    P_{t,t} &= P_{t,t-1} - K_tH_tP_{t,t-1} \label{eq:p_tt} \\
    \hat{\xi}_{t+1,t} &= F_t\hat{\xi}_{t,t} + B_tx_t \label{eq:xi_t1} \\
    P_{t+1,t} &= F_tP_{t,t}F_t^{'}+Q_t \label{eq:filter_end}
\end{align}
$K_t$ is the famous Kalman gain matrix. In this subsection, we assume that initial conditions $\hat{\xi}_{1,0}$ and $P_{1,0}$ are known. The Kalman filter proceeds as follows:
\begin{boenumerate}
    \item Given $\theta$, begin with initial value $\hat{\xi}_{1,0}$ and $P_{1,0}$.
    \item Use equation (\ref{eq:filter_begin}) through (\ref{eq:filter_end}) to calculate $\hat{\xi}_{2,1}$, and $P_{2,1}$.
    \item Repeat step 2 for $t\in\{3, 4, ..., T\}$.
\end{boenumerate}

\subsection{Joseph Form and Numerical Robustness}
We define $P_{t,t}$ as a covariance matrix, which is a symmetric positive semi-definite (PSD) matrix, but in practice, we may get $P_{t,t}$ that is neither symmetric nor PSD, due to rounding errors. Look at equation (\ref{eq:p_tt}) again. The subtraction makes calculating $P_{t,t}$ susceptible to rounding errors, sometimes even resulting in negative semi-definite matrix. I use Joseph Form to numerically enforce symmetric PSD. 

The Joseph form (\cite{joseph_1968}) of $P_{t,t}$ is\footnote{The proof in Appendix \ref{ap:joseph}}:
\[
    P_{t,t} = [I - K_tH_t]P_{t,t-1}[I - K_tH_t]' + K_tR_tK_t^{'}    
\]
If we guarantee $P_{t,t-1}$ and $R_t$ to be PSD, then $P_{t,t}$ is PSD by construction. 

\subsection{Missing Measurements and Sequential Filtering} \label{subsec:seq_filter}
So far we have assume that we observe full measurement $y_t$ for each $t$. If we do not have full observation, we can instead update the Kalman Filter sequentially (see (\cite{durbin_koopman_2001}) section 6.4 for details), based only on observed measurements. Sequential filtering also boosts speed dramatically for in the presence of large measurement dimensions, reducing cost from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. First assume that $R_t$ is diagonal, with noise variance $\sigma_{t:1}^2$ for measurement $i$ at time $t$. Equation (\ref{eq:measure}) then becomes:
\begin{align*}
    y_t &= 
    \begin{pmatrix}
        y_{t:1} \\
        \vdots \\ 
        y_{t:n}
    \end{pmatrix} 
    = \begin{pmatrix}
        H_{t:1}\xi_t + D_{t:1}x_t + w_{t:1} \\
        \vdots \\
        H_{t:n}\xi_t + D_{t:n}x_t + w_{t:n}
    \end{pmatrix}
\end{align*}
where $H_{t:n}$ and $D_{t:n}$ are the $n_{th}$ row of $H_t$ and $D_t$. Define $\hat{\xi}_{t:i}$ as:
\begin{align*}
    \hat{\xi}_{t,t:i} &\equiv E(\xi_t|X_{t-1},Y_{t-1},x_{t:1},...,x_{t:n},y_{t:1},...,y_{t:n};\theta) \\
    P_{t,t:i} &\equiv Var(\xi_t|X_{t-1},Y_{t-1},x_{t:1},...,x_{t:n},y_{t:1},...,y_{t:n};\theta) 
\end{align*}
We initialize the measurement update process with:
\begin{align}
    \hat{\xi}_{t,t:0} = \hat{\xi}_{t,t-1} \label{eq:seq_init1} \\
    P_{t,t:0} = P_{t,t-1} \label{eq:seq_init2}
\end{align}
For each successive measurement $i$, we have:
\begin{align}
    \hat{\xi}_{t,t:i} &= \hat{\xi}_{t,t:i-1} + K_{t:i}(y_{t:i} - H_{t:i}\hat{\xi}_{t,t:i-1} - D_{t:i}x_{t}) \label{eq:seq_start} \\ 
    K_{t:i} &\equiv \frac{P_{t,t:i-1}(H_{t:i})^{'}}{H_{t:i}P_{t,t:i-1}(H_{t:i})^{'}+\sigma_{t:i}^{2}} \\
    P_{t,t:i} &= [I - K_{t:i}H_{t:i}]P_{t,t:i-1}[I-K_{t:i}H_{t:i}]^{'} + K_{t:i}\sigma_{t:i}^{2}(K_{t:i})^{'} \label{eq:seq_end}
\end{align}
where $\sigma_{t:i}^2$ is the $i$-th diagonal value or $R_t$. Note that because $H_{t:i}P_{t,t:i-1}H_{t:i}^{'}+\sigma_{t:i}^{2}$ is a scalar, we don't have to perform matrix inverse, and this formula is agnostic to the number of measurements in $y_t$. Sequential filter proceeds as follows:
\begin{boenumerate}
    \item Initialize with (\ref{eq:seq_init1}) and (\ref{eq:seq_init2})
    \item \label{step:seq_update} Update Kalman Filter using equation (\ref{eq:seq_start}) through (\ref{eq:seq_end})
    \item Repeat Step \ref{step:seq_update} for each $y_{t:i}$
\end{boenumerate}
If $R_t$ is not diagonal, we can use LDL Decomposition\footnote{In practice, I perform LDL Decomposition with \texttt{scipy.linalg.ldl}.} to transform the original LDS into one with independent measurement noise. Given that $R_t$ is PSD, we have $R_t = L_tS_t(L_t^{-1})^{'}$. Pre-multiply equation (\ref{eq:measure}) by $L_t^{-1}$, and we have\footnote{Due to special properties of triangular matrices, I perform matrix inverse on triangular matrix with \texttt{scipy.linalg.lapack.clapack.dtrtri} subroutine.}:
\[
    \tilde{y}_t = \tilde{D}_t\xi_{t} + \tilde{D}_{t}x_t + \tilde{w}_t
\]
where $\tilde{(\cdot)}_t = L_t^{-1}(\cdot)_t$. In the following sections, I will omit the $(\sim)$ sign, and assume $R_t$ is always diagonal.

\subsection{Initialization with Diffuse Filtering}
So far we have assumed that we know values of $\hat{\xi}_{1,0}$ and $P_{1,0}$. In practice, such information are rarely available. If instead, we know that the time series is stationary, we can use the unconditional mean and variance for $\hat{\xi}_{1,0}$ and $P_{1,0}$. If the time series is at least partially non-stationary, then we should assume no prior information on the non-stationary part of $\xi_t$, that is, we should set $\hat{\xi}_{1,0}=0$ and $P_{1,0}=\infty$. A commonly used solution is to set $P_{1,0}$ to some large value (e.g. $10^7$) to approximate infinity, but better algorithms exist for exact initialization. In what follows, I discuss one such algorithm that is implemented by \texttt{linkalman}.

Consider $\hat{\xi}_{1,0}$ again. Following (\cite{koopman_1997}), we define $\hat{\xi}_{1,0}$ as:
\begin{align}
    \hat{\xi}_{1,0} = a + A\eta + \Pi\varepsilon \label{eq:init}
\end{align}
$A$ and $\Pi$ are selection matrices, $\eta\sim\mathcal{N}(0,\kappa I_{\infty})$, and $\varepsilon\sim\mathcal{N}(0,Q_{*})$. Set $0$ for elements in $a$ corresponding to $A$, and stationary means for elements corresponding to $\Pi$. $I_{\infty}$ is an identity matrix with size equal to the number of non-stationary unknowns, and $Q_{*}$ is Essentially, equation (\ref{eq:init}) groups $\hat{\xi}_{1,0}$ into two categories. If $\hat{\xi}_{1,0}^i$, the $i$-th element of $\hat{\xi}_{1,0}$, is non-stationary and not known, it has a distribution $\mathcal{N}(0, \kappa)$. With $\kappa\rightarrow\infty$, $\mathcal{N}(0,\kappa)$ captures the fact that we know nothing about the initial value of a non-stationary process. If on the other hand, the initial state of $\hat{\xi}_{1,0}^i$ is either known or stationary, It has a proper distribution with unconditional mean and covariances\footnote{If $X_t$ is involved in the state transition process then most of the time the process is not stationary.}. 

From equation (\ref{eq:init}), we have:
\begin{align*}
    \hat{\xi}_{1,0} &= a \\
    P_{1,0} &= \kappa P_{\infty} + P_{*}
\end{align*}
where $P_{\infty}=AA^{'}$ and $P_{*}=\Pi Q_0 \Pi^{'}$. By linearity of the filtering process, we can write $P_{t,t-1}$ as:
\begin{align}
    P_{t,t-1} = \kappa P_{\infty,t,t-1} + P_{*,t,t-1} + \mathcal{O}(\kappa^{-1}) \label{eq:P_diffuse}
\end{align}
where $\mathcal{O}(\kappa^{-1})$ is a function $f(\kappa)<\infty$ as $\kappa\rightarrow\infty$. To use the updating procedure described in subsection \ref{subsec:seq_filter}, we need to find the expression for $\Upsilon_t^{-1}$, where $\Upsilon_t\equiv H_tP_{t,t-1}H_t^{'}+R_t$. Following Appendix \ref{ap:init_filter}, We can write $\Upsilon_{t}$ as:
\begin{align*}
    \Upsilon_t = \kappa\Upsilon_{\infty,t} + \Upsilon_{*,t} + \mathcal{O}(\kappa^{-1})
\end{align*}
In what follows, I describe the updating rule for exact initialization, for technical details, please refer to Appendix \ref{ap:init_filter}.

Given $\hat{\xi}_{t,t-1}$ and $P_{t,t-1}=\kappa P_{\infty,t} + P_{*,t} + \mathcal{O}(\kappa^{-1})$, if $\Upsilon_{\infty,t}\neq 0$:
\begin{boenumerate}
    \item Calculate $K_t$ using equation (\ref{eq:K1_diffuse_start}) to (\ref{eq:K1_diffuse_end})
    \item Calculate $\hat{\xi}_{t,t}$ and $P_{t,t}$ from equation (\ref{eq:diff_xi1}) and (\ref{eq:diff_P1})
    \item Calculate $\hat{\xi}_{t+1,t}$ and $P_{t+1,t}$ from equation (\ref{eq:xi_t1}) and (\ref{eq:filter_end})
\end{boenumerate}

If $\Upsilon_{\infty,t}=0$:
\begin{boenumerate}
    \item Calculate $K_t$ using equation (\ref{eq:K2_diffuse})
    \item Calculate $\hat{\xi}_{t,t}$ and $P_{t,t}$ from equation (\ref{eq:diff_xi2}) and (\ref{eq:diff_P2})
    \item Calculate $\hat{\xi}_{t+1,t}$ and $P_{t+1,t}$ from equation (\ref{eq:xi_t1}) and (\ref{eq:filter_end})
\end{boenumerate}

\section{Kalman Smoother} \label{sec:smoother}
\subsection{State Smoother}
In Section \ref{sec:filter}, we use Kalman Filter to find $\{\hat{\xi}_{t,t}, K_t, P_{t,t}, \hat{\xi}_{t,t-1}, P_{t,t-1}\}$ for each $t$. If a dataset is given, we have the entire measurement sequence $Y_T$. Kalman Smoother is a technique of integrating information up to $T$ to infer $\xi_t$ at time $t$, $\hat{\xi}_{t,T}$ and $P_{t,T}$. 

Suppose in addition to state estimates from Kalman Filter, we also know $\hat{\xi}_{t+1,T}$ and $P_{t,T}$. We have\footnote{See Appendix \ref{ap:smooth} for proof.}:
\begin{align}
    \hat{\xi}_{t,T} &= \hat{\xi}_{t,t} + J_t(\hat{\xi}_{t+1,T}-\hat{\xi}_{t+1,t}) \label{eq:smooth_start} \\
    J_t & \equiv P_{t,t}F_t^{'}P_{t+1,t}^{-1} \\
    P_{t,T} &= P_{t,t} + J_t(P_{t+1,T}-P_{t+1,t})J_t^{'} \label{eq:smooth_end}
\end{align}
The procedure for backward smoothing is as follows:
\begin{boenumerate}
    \item Start with $\hat{\xi}_{T,T}$ and $P_{T,T}$ obtained from the filtering process.
    \item \label{step:back_smooth} For $t$, use equation (\ref{eq:smooth_start}) through (\ref{eq:smooth_end}) to get $\hat{\xi}_{t,T}$ and $P_{t,T}$.
    \item Repeat step \ref{step:back_smooth} for $t \in \{1,2,...,T-1\}$.
\end{boenumerate}

(\cite{dejong_1989}) proposes a more efficient approach to calculating state smoothers. Define $d_t$ as:
\begin{align*}
    d_t \equiv y_t - H_t\hat{\xi}_{t,t-1} - D_tX_t
\end{align*}
Note that knowing $\{Y_T, X_T\}$ is equivalent to knowing $\{Y_{t-1},d_t,d_{t+1},...,d_T, X_T\}$. Define $L_t$ as:
\[
    L_t \equiv F_t(I-K_tH_t) 
\]
then use the fact that $d_i \perp d_j \:\forall\: \{i,j\} \in \{t,...,T\}$ and $d_i \perp Y_{t-1}$ and Lemma (\ref{lem:1}), we have:
\begin{align*}
    \hat{\xi}_{t,T} &= E(\xi_t|Y_{t-1},d_{t},...,d_T,X_T;\theta) \\
    &= \hat{\xi}_{t,t-1} + \sum_{j=t}^{T}Cov(\xi_t,d_j)\Upsilon_j^{-1}d_j \\
    &= \hat{\xi}_{t,t-1} + P_{t,t-1}\sum_{j=t}^T\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)H_j^{'}\Upsilon_j^{-1}d_j
\end{align*}
If $j=t$, then we replace the product operation is not carried out and is replace with $I$. Define $r_t$ as:
\begin{align*}
    r_{t-1} \equiv \begin{cases}
        0 & \text{if $t=T+1$} \\
        \sum_{j=t}^T\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)H_j^{'}\Upsilon_j^{-1}d_j & \text{if $t\leq T$}
    \end{cases}
\end{align*}
We can then write backwards recursive formulation for $\hat{\xi}_{t,T}$ as:
\begin{align}
    r_{t-1} &= H_t^{'}\Upsilon_t^{-1}d_t + L_t^{'}r_t \label{eq:r1t} \\
    \hat{\xi}_{t,T} &= \hat{\xi}_{t,t-1} + P_{t,t-1}r_{t-1} \label{eq:smooth_state2}
\end{align}

To calculate $P_{t,T}$, we use Lemma \ref{lem:1} again and have the following result:
\begin{align*}
    P_{t,T} &= P_{t,t-1} - \sum_{j=t}^TCov(\xi_t,d_j)\Upsilon_j^{-1}Cov(\xi_t,d_j)^{'} \\
    &=P_{t,t-1} - P_{t,t-1}\sum_{j=t}^{T}\left[\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)H_j\Upsilon_j^{-1}H_j^{'}\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)^{'}\right]P_{t,t-1}
\end{align*}
Similarly, we can find $P_{t,T}$ through backwards recursions. Define $N_t$ as:
\begin{align*}
    N_{t-1} \equiv \begin{cases}
        0 & \text{if $t=T+1$} \\
        \sum_{j=t}^{T}\left[\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)H_j\Upsilon_j^{-1}H_j^{'}\left(\prod_{i=t+1}^{j}L_{i-1}^{'}\right)^{'}\right] & \text{if $t\leq T$}
    \end{cases}
\end{align*}
Then we have:
\begin{align}
    N_{t-1} &= H_t^{'}\Upsilon_t^{-1}H_t + L_t^{'}N_tL_t \label{eq:N1t} \\
    P_{t,T} &= P_{t,t-1}- P_{t,t-1}N_{t-1}P_{t,t-1} \label{eq:smooth_P2}
\end{align}
We can use equation (\ref{eq:r1t}) through (\ref{eq:smooth_P2}) to perform backwards recursion.

\subsection{Disturbance Smoother}
If the system dynamics, $M_t$ are known, Kalman filters are sufficient in performing predictions, as it does not require measurements. If on the other hand, we do not know $M_t$, then we need Kalman Smoother to integrate information we know about the entire training data and infer $M_t$. In this subsection, I will discuss disturbance smoother, a variation of the Kalman smoother that provides more efficient smoothed state estimates and clean forms for calculating likelihoods\footnote{This methods does not provide estimates of $P_{t,T}$. If finding $P_{t,T}$ is the goal, use equation (\ref{eq:r1t}) through (\ref{eq:smooth_P2}) instead}. It is also the algorithm that I use in \texttt{linkalman}. 

Consider the smoothed state disturbances $\hat{v}_{t,T}$, and measurement disturbances $\hat{w}_{t,T}$. Using Lemma \ref{lem:1}, we have:
\begin{align}
    \hat{w}_{t,T} &= E(w_t|Y_{t-1}, d_t,...d_T,X_T;\theta) \nonumber \\
    &= \sum_{j=t}^TE(w_t,d_j^{'})\Upsilon_j^{-1}d_j \nonumber \\
    &= R_t\Upsilon_t^{-1}d_t - R_tK_t^{'}F_t^{'}\sum_{j=t+1}^T\left[\prod_{i=t+2}^j\left(L_{i-1}^{'}\right)H_j^{'}\Upsilon_j^{-1}d_j\right] \nonumber \\
    &= R_t(\Upsilon_t^{-1}d_t - K_t^{'}F_t^{'}r_t) \nonumber \\
    &= R_t\Upsilon_t^{-1}(d_t - H_tP_{t,t-1}F_t^{'}r_t) \label{eq:disturb_w}
\end{align}

To calculate $W_t \equiv Cov(w_t|Y_T,X_T;\theta)$, we again invoke Lemma \ref{lem:1} and obtain:
\begin{align}
    W_t &= R_t - \sum_{j=t}^TCov(w_t,d_j)\Upsilon_j^{-1}Cov(w_t,d_j)^{'} \nonumber \\
    &= R_t - R_t(\Upsilon_t^{-1}+K_t^{'}F_t^{'}N_tF_tK_t)R_t \label{eq:disturb_W}
\end{align}

Similarly, we can express $\hat{v}_{t,T}$ and $V_t \equiv Cov(v_t|Y_T,X_T;\theta)$ as:
\begin{align}
    \hat{v}_{t,T} &= \sum_{j=t}^{T}E(v_td_t^{'})\Upsilon_j^{-1}d_j \nonumber \\
    &= Q_tr_t \label{eq:disturb_v} \\
    V_t &= Q_t - \sum_{j=t}^{T}Cov(v_t, d_j)\Upsilon_j^{-1}Cov(v_t,d_j)^{'} \nonumber \\
    &= Q_t - Q_tN_tQ_t \label{eq:disturb_V}
\end{align}

Using equation (\ref{eq:disturb_w}) through (\ref{eq:disturb_V}), we can get smoothed disturbances.

\subsection{Sequential Smoother}
As with the case of Kalman filtering, calculating $\Upsilon_t^{-1}$ is expensive and subject to failure due to rounding errors. \cite{durbin_koopman_2000} propose sequential smoothing to greatly improved the efficiency and robustness of Kalman Smoothers. The univariate approach in Kalman filter essentially treats observations flowing in one at a time, so we can readily adapt our smoother.

Define $y_{t,i}$ as the $i$-th non-missing measurement\footnote{Note that $i$ may not correspond to the index of the measurement if some are missing.} of $y_t$, we have:
\begin{align}
    r_{t:i-1} &= \begin{cases}
        \frac{H_{t:i}^{'}d_{t:i}}{\Upsilon_{t:i}} + L_{t:i}^{'}r_{t:i} \\
        F_{t-1}^{'}r_{t:0} 
    \end{cases} \label{eq:seq_r} \\
    \Upsilon_{t:i} &= H_{t:i}P_{t,t:i-1}H_{t:i}^{'}+\sigma_{t:i}^2 \nonumber \\
    L_{t:i} &= F_t(I-K_{t:i}H_{t:i}) \nonumber \\
    N_{t:i-1} &= \begin{cases}
        \frac{H_{t:i}^{'}H_{t:i}}{\Upsilon_{t:i}} + L_{t:i}^{'}N_{t:i}L_{t:i} \\
        F_{t-1}^{'}N_{t:0}F_{t-1}
    \end{cases} \label{eq:N_seq}
\end{align}

For disturbance smoother, we may calculate $\hat{v}_{t,T:i}$, $V_{t:i}$, $\hat{w}_{t,T:i}$, and $W_{t:i}$ as:
\begin{align}
    \hat{w}_{t,T:i} &= R_{t:i}(\frac{d_{t:i}}{\Upsilon_{t:i}}-K_{t:i}^{'}H_{t:i}^{'}r_{t:i}) \\
    W_{t:i} &= R_{t:i} - R_{t:i}^2(\Upsilon_{t:i}^{-1}+K_{t:i}^{'}F_{t:i}^{'}N_{t:i}F_{t:i}^{'}K_{t:i})  \\
    \hat{v}_{t,T:i} &= Q_tr_{t:i} \\
    V_{t:i} &= Q_t - Q_tN_{t:i}Q_t
\end{align}

\subsection{Rounding Errors and Nearest PSD}
Note that for Kalman Smoother, we don't have Joseph Form formulation, and therefore we may still get non-PSD covariance matrix and cause the smoother to fail. If a matrix is symmetric, we may check PSD using Cholesky Decomposition, which is efficient under symmetry conditions. Since we are using sequential techniques, we avoid asymmetric covariance matrices due to rounding errors of matrix inversion and are able to guarantee symmetry throughout our operations. 

If a covariance matrix $A$ is not PSD, we can use the Nearest PSD matrix with the techniques developed in \cite{higham_1988}. Omitting technical details, it amounts to replacing $A$ with $\bar{A}$, where $\bar{A}$ is be calculated as:
\begin{align*}
    A &= USV^{'} \\
    \Sigma &= VSV^{'} \\
    \bar{A} &= \frac{\Sigma + \Sigma^{'} + A + A^{'}}{4}
\end{align*}

\subsection{Diffuse Smoothing}

\section{EM Algorithm} \label{sec:EM}
Kalman Filter and Kalman Smoother are useful to predict measurements if we know $\theta$. Quite often, we need to estimate $\theta$ as well. A popular method that I implement here is EM algorithm\footnote{There are many other algorithms as well. For example, Newton-Raphson, and MCMC algorithm. EM algorithm is notable for its ease to implement. But it is difficult to generate a confidence interval of the parameters, unlike methods such as Newton-Raphson algorithm. For large enough time series data and decent parameter dimensions, this negligence does not affect the confidence of the prediction by much.}. For general proof, one can refer to Appendix \ref{ap:EM_proof}.

The log likelihood function for a HMM system is:
\begin{align*}
    L(Y_T,X_T, \theta) &= log[\mathbb{P}(Y_T|X_T,\theta) \\
\end{align*}
Following equation (\ref{eq:Q}), we maximize $L(Y_T,X_T,\theta)$ by maximizing: 
\begin{align*}
    G(Y_T,X_T,\theta) = \int log[\mathbb{P}(Y_T,\Xi_T|X_T,\theta)]\mathbb{P}(\Xi_T|Y_T,X_T,\theta)d\Xi_T 
\end{align*}
Denote $G(\theta,\theta_1)$ as: 
\[
    G(\theta,\theta_i) \equiv \int log[\mathbb{P}(Y_T,\Xi_T|X_T,\theta)]\mathbb{P}(\Xi_T|Y_T,X_T,\theta_i)d\Xi_T
\]
EM algorithm proceeds as follows:
\begin{boenumerate}
    \item Start with initial parameter value $\theta_0$
    \item \label{step:EM_E} For iteration $i$, use Kalman Smoother to get $\mathbb{P}(\Xi_T|Y_T,X_T,\theta_{i-1})$
    \item \label{step:EM_M} Find $\theta_{i}$ that maximizes $G(\theta,\theta_{i-1})$    
    \item Repeat step \ref{step:EM_E} and \ref{step:EM_M} until $\{G(Y_T,X_T,\theta_i)\}_i$ converges to a local optimal.
\end{boenumerate}

\subsection{$G(Y_T,X_T,\theta)$ with Missing Measurements} \label{subsec:G}
If we know $(Y_T,\Xi_T)$, by Markov Property, we can rewrite $log[\mathbb{P}(Y_T,\Xi_T|X_T,\theta)]$ as:
\begin{align}
    log[\mathbb{P}(Y_T,\Xi_T|X_T,\theta)] &= \sum_{t=1}^{T}log[\mathbb{P}(\xi_t|\xi_{t-1},x_{t-1},\theta)] 
    + \sum_{t=1}^{T}log[\mathbb{P}(y_t|\xi_t,x_t,\theta)] \nonumber \\
    &= log[\mathbb{P}(\xi_1|\theta)] + \sum_{t=2}^{T}log[\mathbb{P}(\xi_t|\xi_{t-1},x_{t-1},\theta)] \nonumber \\ 
    &+ \sum_{t=1}^{T}log[\mathbb{P}(y_t|\xi_t,x_t,\theta)]\label{eq:log}
\end{align}
For $t>1$, the second term in equation (\ref{eq:log}) is:
\begin{align}
    log[\mathbb{P}(\xi_t|\xi_{t-1},x_{t-1},\theta)] &= const-\frac{1}{2}log(|Q_t|) 
    -\frac{1}{2}\delta_t^{'}Q_t^{-1}\delta_t \nonumber \\
    &= const-\frac{1}{2}log(|Q_t|) 
    -\frac{1}{2}Tr[\delta_t^{'}Q_t^{-1}\delta_t] \nonumber \\
    &= const-\frac{1}{2}log(|Q_t|) 
    -\frac{1}{2}Tr[Q_t^{-1}\delta_t\delta_t^{'}] \label{eq:log1} \\
    \delta_t &\equiv \xi_t - F_{t-1}\xi_{t-1}-B_{t-1}x_{t-1} \nonumber
\end{align}
The second equality in equation (\ref{eq:log1}) holds because $Tr(a)=a$ for scalar $a$. The third equality holds because $Tr(AB)=Tr(BA)$. 

We may calculate:
\begin{align}
    G_1^{t}(\theta,\theta_{i-1}) &\equiv \int log[\mathbb{P}(\xi_t|\xi_{t-1},\theta)]\mathbb{P}(\Xi_T|Y_T,X_T,\theta_{i-1})d\Xi_T \nonumber \\
    &= const -\frac{1}{2}log(|Q_t|)-\frac{1}{2}Tr[E(Q_t^{-1}\delta_t\delta_t^{'}|Y_T,X_T,\theta_{i-1})] \nonumber \\
    &= const - \frac{1}{2}log(|Q_t|) - \frac{1}{2}Tr[Q_t^{-1}E(\delta_t\delta_t^{'}|Y_T,X_T,\theta_{i-1})] \label{eq:log1_trace}
\end{align}
Note that when $t=1$, we have $\xi_1\sim\mathcal{N}(\hat{\xi}_{1,0}, P_{1,0})$. We may calculate:
\begin{align}
    G_1^1(\theta,\theta_{i-1}) &= const - \frac{1}{2}log(|P_{1,0}|) \nonumber \\
    &- \frac{1}{2}Tr\{P_{1,0}^{-1}E[\delta_1\delta_1^{'}|Y_T, X_T, \theta_{i-1}]\} \nonumber \\
    \delta_1 &\equiv \xi_1 - \hat{\xi}_{1,0} \nonumber
\end{align}
Detailed derivation of $E(\delta_t\delta_t^{'}|Y_T,X_T,\theta_{i-1})$ is given in Appendix \ref{ap:log}. If $Q_t$ does not have full rank, I use \texttt{scipy.linalg.pinvh} to find the pseudo inverse of $Q_t$, or \texttt{scipy.linalg.eigh}\footnote{After finding all eigenvalues of $Q_t$, multiply non-zero eigenvalues to get the determinant.} to find the pseudo determinant of $Q_t$.

Calculating conditional expectation of the second term in equation (\ref{eq:log}) is similar. Let's consider:
\[
    G_2^t(\theta,\theta_{i-1}) \equiv \int log[\mathbb{P}(y_t|\xi_{t},x_t, \theta)]\mathbb{P}(\Xi_T|Y_T,X_T,\theta_{i-1})d\Xi_T 
\]
With the assumption of Gaussian white noises, we have:
\begin{align}
    G_2^t(\theta,\theta_{i-1}) &= const - \frac{1}{2}log(|R_t|)-\frac{1}{2}Tr[R_t^{-1}E(\chi_t\chi_t^{'}|Y_T,X_T,\theta_{i-1})] \label{eq:log2_trace} \\
    \chi_t &\equiv y_t - H_t\xi_t - D_tx_t \nonumber
\end{align}
See Appendix \ref{ap:log2} for detailed derivation of $G_2^t(\theta,\theta_{i-1})$. Now $G(\theta,\theta_{i-1})$ is:
\begin{align*}
    G(\theta,\theta_{i-1}) &= \sum_{t=1}^{T}G_1^t(\theta,\theta_{i-1}) + \sum_{t=1}^{T}G_2^t(\theta,\theta_{i-1})
\end{align*}
We can use numerical methods\footnote{If system matrices are constant, we have a closed form solution. But I decide to use numerical optimizations so that the EM is also able to solve HMM with complex system matrices.} to find $\theta_i$ that maximizes $G(\theta,\theta_{i-1})$.

\section{\texttt{linkalman}} \label{sec:codebase}

\section{Application} \label{sec:apply}
In this section, I use the Kalman Filter to solve a complex time series problem from (\cite{brodersen_etal_2015}) (with some modification). Consider the following time series: 
\begin{align}
    g_t &= \mu_t + \gamma_t + D_1x_t + w_{g,t} \label{eq:ts_setup} \\
    e_t &= \pi(\mu_t + \gamma_t) + D_2x_t + w_{e_t}
\end{align}
In equation (\ref{eq:ts_setup}), $g_t$ is the daily S\&P index, and $e_t$ is gold price index at time $t$, both of wich can be viewed as measurements of economic status at time $t$. We consider $g_t$ as the unbised measurement of $(\mu_t+\gamma_t)$, and $e_t$ is a biased measurement. $x_t$ includes $1$ as well as whether it is a holidy at time $t$. $\mu_t$ is the trend component, and $gamma_t$ is the seasonal component. $\mu_t$ has the following specification:
\begin{align}
    &\mu_{t+1} = \mu_{t} + \delta_t + \eta_{\mu,t+1} \\
    &\delta_{t+1} = \Delta_{\delta} + \rho_{\delta}(\delta_t - \Delta_{\delta}) + \eta_{\delta,t+1} 
\end{align}
where $\eta_{\mu,t}\sim\mathcal{N}(0,\sigma_{\mu}^2)$, $\eta_{\delta,t}\sim\mathcal{N}(0,\sigma_{\delta}^2)$. $\delta_t$ is interpreted as the slop of the trend and is assumed to be an AR(1) variation around a long-term slope of $\Delta_{\delta}$. $\gamma_t$ has the following specification:
\begin{align}
    \sum_{j=0}^6\gamma_{t-j} &= \eta_{\gamma,t}
\end{align}
where $\eta_{\gamma,t}\sim\mathcal{N}(0,\sigma_{\gamma}^2)$. We further assume that $\eta_{\mu,t},\eta_{\delta,t},\eta_{\gamma,t}$ are mutually independent. 

To fit the LDS model Kalman Filter, we rewrite it as:
\begin{align*}
    \xi_t = \begin{pmatrix}
        \mu_t \\
        \delta_t \\
        \gamma_t \\
        \gamma_{t-1} \\
        \gamma_{t-2} \\
        \gamma_{t-3} \\
        \gamma_{t-4} \\
        \gamma_{t-5} \\
        \gamma_{t-6}
    \end{pmatrix}, 
    &\ 
    F_t = F = \begin{pmatrix}
        1 & 1  & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & \rho_{\delta}  & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & -1 & -1 & -1 & -1 & -1 & -1 & 0 \\
        0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 0 & 1 & 0
    \end{pmatrix} \\
    v_t = \begin{pmatrix}
        \eta_{\mu,t} \\
        \eta_{\delta,t} \\
        \eta_{\gamma,t} \\
        0 \\
        0 \\
        0 \\
        0 \\
        0 \\
        0 \\
    \end{pmatrix},
    &\ 
    B_t = B = \begin{pmatrix}
        0 & 0 \\
        (1-\rho_{\delta})\Delta_{\delta} & 0 \\
        0 & 0 \\
        0 & 0 \\
        0 & 0 \\
        0 & 0 \\
        0 & 0 \\
        0 & 0 \\
        0 & 0 
    \end{pmatrix},
    \ 
    x_t = \begin{pmatrix}
        1 \\
        \mathbbm{1}_{holiday}
    \end{pmatrix} \\
    y_t = \begin{pmatrix}
        g_t \\
        e_t 
    \end{pmatrix},
    &\ 
    D_t = D = \begin{pmatrix}
        0 & h_1 \\
        a & h_2
    \end{pmatrix},
    \ 
    w_t = \begin{pmatrix}
        w_{1,t} \\
        w_{2,t}
    \end{pmatrix} 
    , \ w_t\sim\mathcal{N}(0,R) \\
    & H_t = H = \begin{pmatrix}
        1 & 1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\
        \pi & \pi & 0 & 0 & 0 & 0 & 0 & 0 & 0
    \end{pmatrix}
\end{align*}
The system paramter is $\theta\equiv(\hat{\xi}_{1,0}, P_{1,0}, \rho_{\delta}, \Delta_{\delta}, Q, R, a, h_1, h_2, \pi)$, where $Q$ takes the form:
\begin{align*}
    Q \equiv \begin{pmatrix}
        \sigma_{\mu}^2 & 0 & 0 & \cdots & \cdots & 0 \\
        0 & \sigma_{\delta}^2 & 0 & & & \vdots \\
        0 & 0 & \sigma_{\gamma}^2 & & & \vdots \\
        \vdots & & & 0 & & \vdots \\
        \vdots & & & & \ddots & \vdots \\
        0 & \cdots & \cdots & \cdots & \cdots & 0
    \end{pmatrix}
\end{align*}
We may use the standard EM algorithm to solve $\theta$.

\printbibliography
\pagebreak
\appendix

\section{Derivation of $\hat{\xi}_{t,t}$ and $P_{t,t}$} \label{ap:iter_proj}
\begin{lemma}[Law of Iterated Projections] \label{lem:1}
    Let $\mathcal{P}(Y_3|Y_2,Y_1)$ be the projection of $Y_3$ on $(Y_2, Y_1)$. Denote $\Omega_{ij}$ as $\Omega_{ij} = E(Y_iY_j^{'})$, then we have projections:
    \[
        \mathcal{P}(Y_3|Y_2,Y_1) = \mathcal{P}(Y_3|Y_1)+H_{32}H_{22}^{-1}[Y_2 - \mathcal{P}(Y_2|Y_1)]
    \]
    and variance matrix:
    \[
        Var(Y_3|Y_2,Y_1) = H_{33} - H_{32}H_{22}^{-1}H_{32}^{'}
    \]
    where 
    \begin{align*}
        H_{22} &= E\{[Y_2-\mathcal{P}(Y_2|Y_1)][Y_2-\mathcal{P}(Y_2|Y_1)]^{'}\} \\
        H_{23} &= H_{32}^{'} = E\{[Y_2-\mathcal{P}(Y_2|Y_1)][Y_3-\mathcal{P}(Y_3|Y_1)]^{'}\} \\
        H_{33} &= E\{[Y_3-\mathcal{P}(Y_3|Y_1)][Y_3-\mathcal{P}(Y_3|Y_1)]^{'}\}
    \end{align*}
\end{lemma}

Let $\xi_t$ as $Y_3$, $y_t$ as $Y_2$, and $(x_t,Y_{t-1})$ as $Y_1$, we obtain formula for $\hat{\xi}_{t,t}$ and $P_{t,t}$.

\section{Derivation of Joseph Form of $P_{t,t}$} \label{ap:joseph}
\begin{align*}
    P_{t,t} &= [I - K_tH_t]P_{t,t-1}[I - K_tH_t]' + K_tR_tK_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + K_tH_tP_{t,t-1}H_t^{'}K_t^{'} + K_tR_tK_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + K_t(H_tP_{t,t-1}H_t^{'} + R_t)K_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + P_{t,t-1}H_t^{'}K_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1}
\end{align*}
The fourth equality follows from equation (\ref{eq:gain}).

\section{Proof of Backward Smoothing} \label{ap:smooth}
Consider the following:
\begin{align*}
    \hat{\xi}_{t,t+1,t} &= \hat{\xi}_{t,t} + \{E[(\xi_t-\hat{\xi}_{t,t})(\xi_t-\hat{\xi}_{t+1,t})^{'}]\} \\
    & \times \{E[(\xi_{t+1}-\hat{\xi}_{t+1,t})(\xi_{t+1}-\hat{\xi}_{t+1,t})^{'}]\}^{-1} \\
    & \times (\xi_{t+1} - \hat{\xi}_{t+1,t}) \\
    &= \hat{\xi}_{t,t} + P_{t,t}F_t^{'}P_{t+1,t}^{-1}(\xi_{t+1}-\hat{\xi}_{t+1,t}) \\
    &= \hat{\xi}_{t,t} + J_t(\xi_{t+1}-\hat{\xi}_{t+1,t})
\end{align*}
The first equality follows from Lemma \ref{lem:1}. Note that we assume $w_t$ and $v_t$ to be white noises, then we have:
\[
    \hat{\xi}_{t,t+1,t} = \hat{\xi}_{t,t+1,T}
\]
Now take expectation over $\xi_{t+1}$, we have:
\[
    \hat{\xi}_{t,T} = \hat{\xi}_{t,t} + J_t(\hat{\xi}_{t+1,T}-\hat{\xi}_{t+1,t})
\]
With some algebra, we can get formula for $P_{t,T}$.

\section{EM Algorithm Premier:} \label{ap:EM_proof}
Denote $Y$ as observed measurements, $\xi$ as hidden states, and $\theta$ as parameters to be estimated. We want to optimize:
\begin{align}
    L(\theta) & = log[P(Y|\theta)] \nonumber \\
    & = log\left[\frac{P(Y,\xi|\theta)}{P(\xi|Y,\theta)}\right] \nonumber \\
    & = log[P(Y,\xi|\theta)] - log[P(\xi|Y,\theta)] \label{eq:general_mle}
\end{align}

Take expectation of equation (\ref{eq:general_mle}) wrt. some distribution of $\xi$ with pdf $f(\xi)$ and get:

\begin{align}
    L(\theta) = & \int f(\xi)log[P(Y,\xi|\theta)]d\xi \nonumber \\
    & - \int f(\xi)log[P(\xi|Y,\theta)]d\xi \nonumber
\end{align}
To optimize $L(\theta)$ we can iterate through the E steps and M steps to achieve a local maximum. By Jensen's inequality, we have the second term in equation (\ref{eq:general_mle}) maximized when $f(\xi)=P(\xi|Y,\theta)$ (E-step). If we define:
\begin{align}
    Q(\theta) = \int log[P(Y,\xi|\theta)]f(\xi|Y,\theta)d\xi \label{eq:Q}
\end{align}
then maximizing $Q(\theta)$ is equivalent to maximizing $ L(\theta)$. For a given $\hat{\theta}$ and $P(\xi|Y, \hat{\theta})$, we find $\theta$ to optimize the first term (M-step). Use the new $\theta$ as $\hat{\theta}$ for the next iteration, and we will reach a local maximum. It is important to note that for a given $\hat{\theta}$, $f(\xi|Y, \hat{\theta})$ is a given quantity and does not change wrt. $\theta$. 

\section{Derivation of Log-likelihood for $\Xi_T$} \label{ap:log}
If $t>1$, recall that $\delta_t \equiv \xi_t - F_{t-1}\xi_{t-1} - B_{t-1}x_{t-1}$. Expanding the expectation terms in equation (\ref{eq:log1_trace}), and denoting $\Theta \equiv (Y_T,X_T, \theta_i)$, we have:
\begin{align*}
    E(\delta_t\delta_t^{'}|\Theta) &= E[(\xi_t-F_{t-1}\xi_{t-1}-B_{t-1}x_{t-1}) 
    (\xi_t-F_{t-1}\xi_{t-1}-B_{t-1}x_{t-1})^{'}|\Theta] \\
    &= E(\xi_t\xi_t^{'}|\Theta) - F_{t-1}E(\xi_{t-1}\xi_{t}^{'}|\Theta) - B_{t-1}x_{t-1}E(\xi_t^{'}|\Theta) \\
    &- E(\xi_t\xi_{t-1}^{'}|\Theta)F_{t-1}^{'} + F_{t-1}E(\xi_{t-1}\xi_{t-1}^{'}|\Theta)F_{t-1}^{'}
    +B_{t-1}x_{t-1}E(\xi_{t-1}^{'}|\Theta)F_{t-1}^{'} \\
    &- E(\xi_t|\Theta)x_{t-1}^{'}B_{t-1}^{'} + F_{t-1}E(\xi_{t-1}|\Theta)x_{t-1}^{'}B_{t-1}^{'}
    +B_{t-1}x_{t-1}x_{t-1}^{'}B_{t-1}^{'}
\end{align*}
We already derive expression for $E(\xi_t|\Theta)$ in (\ref{eq:filter_begin}). In addition, we need to calculate $E(\xi_t\xi_t^{'}|\Theta)$ and $E(\xi_t\xi_{t-1}^{'}|\Theta)$. $E(\xi_t\xi_t^{'}|\Theta)$ is easy to  get:
\begin{align*}
    E(\xi_t\xi_t^{'}|\Theta) & = E(\xi_t|\Theta)E(\xi_t^{'}|\Theta) + Var(\xi_t|\Theta) \\
    &= \hat{\xi}_{t,T}(\hat{\xi}_{t,T})^{'} + P_{t,T}
\end{align*}
To find $E(\xi_t\xi_{t-1}^{'}|\Theta)$, consider the following\footnote{This derivation adapts the general strategy in deriving $\hat{\xi}_{t,T}$ in Chapter 13 of (\cite{hamilton_1994}).}:
\begin{align}
    E(\xi_t\xi_{t-1}^{'}|\xi_t,y_{t-1},X_T,\theta_i) &= \xi_tE(\xi_{t-1}^{'}|\xi_t,y_{t-1},X_T,\theta_i) \nonumber \\
    &= \xi_t(\hat{\xi}_{t-1,t-1})^{'} + \xi_t\xi_t^{'}J_{t-1}^{'} - \xi_{t}(\hat{\xi}_{t,t-1})^{'}J_{t-1}^{'} \label{eq:xi_t,t-1}
\end{align}
By the nature of Markov Dynamics, adding $\{y_t, y_{t+1}, ..., y_T\}$ does not change the value of $E(\xi_t\xi_{t-1}^{'}|\xi_t,y_{t-1},x_{t-1},\theta_i)$. Taking expectations of equation (\ref{eq:xi_t,t-1}) over $\xi_t$, we have:
\begin{align*}
    E(\xi_t\xi_{t-1}^{'})|\Theta) &= \hat{\xi}_{t,T}(\hat{\xi}_{t-1,t-1})^{'} + E(\xi_t\xi_t^{'}|\Theta)J_{t-1}^{'}
    -\hat{\xi}_{t,T}(\hat{\xi}_{t,t-1})^{'}J_{t-1}^{'} \\
    &= \hat{\xi}_{t,T}(\hat{\xi}_{t-1,t-1})^{'} + [\hat{\xi}_{t,T}(\hat{\xi}_{t,T})^{'} + P_{t,T} 
    -\hat{\xi}_{t,T}(\hat{\xi}_{t,t-1})^{'}]J_{t-1}^{'} \\
    &= \hat{\xi}_{t,T}(\hat{\xi}_{t-1,t-1})^{'} + [P_{t,T} + \hat{\xi}_{t,T}(\hat{\xi}_{t,T}-\hat{\xi}_{t,t-1})^{'}]J_{t-1}^{'}
\end{align*}
If $t=1$, we have:
\begin{align*}
    E(\delta_1\delta_1^{'}) &= E(\xi_1\xi_1^{'}|\Theta) - \hat{\xi}_{1,0}(\hat{\xi}_{1,T})^{'}
    -\hat{\xi}_{1,T}(\hat{\xi}_{1,0})^{'} + \hat{\xi}_{1,0}(\hat{\xi}_{1,0})^{'}
\end{align*}

\section{Derivation of Log-likelihood for $Y_T$} \label{ap:log2}
Recall that $\chi_t \equiv y_t - H_t\xi_t - D_tx_t$. Expanding the expectation terms in equation (\ref{eq:log2_trace}), and denoting $\Theta \equiv (Y_T,X_T,\theta_i)$, we have:
\begin{align*}
    E(\chi_t\chi_t^{'}|\Theta) &= E[(y_t - H_t\xi_t - D_tx_t)(y_t - H_t\xi_t - D_tx_t)^{'}|\Theta] \\
    &= (y_t-D_tx_t)(y_t-D_tx_t)^{'} - H_tE(\xi_t|\Theta)(y_t-D_tx_t)^{'} \\
    &-(y_t-D_tx_t)E(\xi_t^{'}|\Theta)H_t^{'} + H_tE(\xi_t\xi_t^{'}|\Theta)H_t^{'}
\end{align*}
Now look at equation (\ref{eq:log2_trace}) again. Let $M_t$ be:
\begin{align*}
    M_t \equiv Tr[R_t^{-1}E(\chi_t\chi_t^{'}|Y_T,X_T,\theta_{i-1})] 
\end{align*}
If $y_t$ has no missing measurements, it is straightforward to calculate $M_t$. If $y_t$ has missing measurements, but $R_t^{-1}$ is diagonal, we can follow (\cite{shumway_stoffer_1982}) and modify $R_t$, $H_t$, $D_t$ and $\chi_t$, which is done in section (\ref{sec:filter}). In fact, we are calculating the marginal distribution of the observed components. 

\section{Sketch Proof of Diffuse Kalman Filter} \label{ap:init_filter}
This section derives from Section 5 of \cite{durbin_koopman_2001}, but provides an alternative formulation using the Joseph form. For original proof without the Joseph formulation, refer \cite{durbin_koopman_2001}. The key intuition is to give an exact result for the initial state distribution with arbitrarily large but finite variances, then find out the limit as the variances approach infinity. Given definition of $P_{t,t-1}$ in equation (\ref{eq:P_diffuse}), and linearity of Kalman filtering, we have:
\begin{align*}
    \Upsilon_t &= \kappa \Upsilon_{\infty,t} + \Upsilon_{*,t} + \mathcal{O}(\kappa^{-1}) \\
    \Upsilon_{\infty,t} &= H_tP_{\infty,t}H_t^{'} \\ 
    \Upsilon_{*,t} &= H_tP_{*,t}H_t^{'} + R_t
\end{align*}
We are interested in finding $\Upsilon_t^{-1}$. Expand $\Upsilon_t^{-1}$ as a power series in $\kappa^{-1}$, we have:
\begin{align*}
    \Upsilon_t^{-1} = \Upsilon_t^{(0)} + \kappa^{-1}\Upsilon_t^{(1)} + \kappa^{-2}\Upsilon_t^{(2)}+\mathcal{O}(\kappa^{-3})
\end{align*}
where $\Upsilon_t^{(i)}$ is the $i$-th term associated with the power series expansion. We collapse other terms of the expansion to $\mathcal{O}(\kappa^{-3})$ as they will converge to 0 as $\kappa \rightarrow \infty$. We find $\Upsilon_t^{(i)}$ by using the fact that: 
\begin{align}
    I =& \Upsilon_t\Upsilon_t^{-1} \nonumber \\
    =& (\kappa \Upsilon_{\infty,t} + \Upsilon_{*,t} + \kappa^{-1}\Upsilon_{a,t} + \kappa^{-2}\Upsilon_{b,t}) \nonumber \\
    &\times (\Upsilon_t^{(0)} + \kappa^{-1}\Upsilon_t^{(1)} + \kappa^{-2}\Upsilon_t^{(2)}+\mathcal{O}(\kappa^{-3})) \label{eq:inverse_diff}
\end{align}

Solving equation (\ref{eq:inverse_diff}), we have:
\begin{align}
    \Upsilon_{\infty,t}\Upsilon_t^{(0)} = 0 \label{eq:upsilon_begin} \\
    \Upsilon_{*,t}\Upsilon_t^{(0)} + \Upsilon_{\infty,t}\Upsilon_t^{(1)} = I \\
    \Upsilon_{a,t}\Upsilon_t^{(0)} + \Upsilon_{*,t}\Upsilon_t^{(1)} + \Upsilon_{\infty,t}\Upsilon_t^{(2)} = 0 \label{eq:upsilon_end}
\end{align}
For a full treatment when $\Upsilon_t$ is not 1-by-1 matrix, see \cite{koopman_1997}. If we are combine initialization with sequential filtering/smoothing, the solution to equation (\ref{eq:upsilon_begin}) to (\ref{eq:upsilon_end}) is much simpler and will be shown below.

If $\Upsilon_{\infty,t}\neq 0$, we have:
\begin{align*}
    \Upsilon_t^{(0)} &= 0 \\
    \Upsilon_t^{(1)} &= \Upsilon_{\infty,t}^{-1} \\
    \Upsilon_t^{(2)} &= -\Upsilon_{\infty,t}^{-1}\Upsilon_{*,t}\Upsilon_{\infty,t}^{-1}
\end{align*}
By the fact that $K_t = P_{t,t-1}H_t^{'}\Upsilon_t^{-1}$, we can write $K_t$ as:
\begin{align}
    K_t &= K_t^{(0)} + \kappa^{-1}K_t^{(1)} + \mathcal{O}(\kappa^{-2}) \label{eq:K1_diffuse_start} \\
    K_t^{(0)} &= P_{\infty,t}H_t^{'}\Upsilon_t^{(1)} \\
    K_t^{(1)} &= P_{*,t}H_t^{'}\Upsilon_t^{(1)} + P_{\infty,t}H_t^{'}F_t^{(2)} \label{eq:K1_diffuse_end}
\end{align}

Now we can express $\hat{\xi}_{t,t}$ as:
\begin{align}
    \hat{\xi}_{t,t} = \hat{\xi}_{t,t-1} + K_t^{(0)}(y_t-H_t\hat{\xi}_{t,t-1}-D_tx_t) + \mathcal{O}(\kappa^{-1}) \label{eq:diff_xi1}
\end{align}

After a lot of algebra, we can also express $P_{t,t}$, in Joseph form, as:
\begin{align}
    P_{t,t} =& \kappa (I-K_t^{(0)}H_t)P_{\infty,t}(I-K_t^{(0)}H_t)^{'} \nonumber \\
    &+(I-K_t^{(0)}H_t)P_{*,t}(I-K_t^{(0)}H_t)^{'} \label{eq:diff_P1} \\
    &+K_t^{(0)}R_t(K_t^{(0)})^{'} + \mathcal{O}(\kappa^{-1}) \nonumber
\end{align}

If $\Upsilon_{\infty,t}=0$, it is easier to get $\hat{\xi}_{t,t}$ and $P_{t,t}$:
\begin{align}
    K_t =& P_{*,t}H_t^{'}\Upsilon_{*,t}^{-1} + \mathcal{O}(\kappa^{-1}) \nonumber \\
    =& K_t^{(*)} + \mathcal{O}(\kappa^{-1}) \label{eq:K2_diffuse} \\
    \hat{\xi}_{t,t} =& \hat{\xi}_{t,t-1} + K_t^{(*)}(y_t - H_t\hat{\xi}_{t,t-1} - D_tX_t) + \mathcal{O}(\kappa^{-1}) \label{eq:diff_xi2}
\end{align}

For $P_{t,t}$, we have:
\begin{align}
    P_{t,t} =& \kappa(I-K_tH_t)P_{\infty,t}(I-K_tH_t)^{'} \nonumber \\
    &+ (I-K_t^{(*)}H_t)P_{*,t}(I-K_t^{(*)}H_t)^{'} \nonumber \\
    &+ K_t^{(*)}R_t(K_t^{(*)})^{'} + \mathcal{O}(\kappa^{-1}) \nonumber \\
    =& \kappa P_{\infty, t} + (I-K_t^{(*)}H_t)P_{*,t}(I-K_t^{(*)}H_t)^{'} \nonumber \\
    &+ K_t^{(*)}R_t(K_t^{(*)})^{'} + \mathcal{O}(\kappa^{-1}) \label{eq:diff_P2}
\end{align}
The second equality follows becasuse $\Upsilon_{\infty,t}=0$.

\section{Sketch Proof of Diffuse Kalman Smoother} \label{ap:init_smoother}


\end{document}
