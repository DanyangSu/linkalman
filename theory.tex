\documentclass[12pt]{article}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage[a4paper, total={6in, 8in}]{geometry}
\usepackage[style=authoryear,sorting=nyt]{biblatex}
\addbibresource{ref.bib}

\newenvironment{boenumerate}
    {\begin{enumerate}\renewcommand\labelenumi{\textbf\theenumi}}
    {\end{enumerate}}
\numberwithin{equation}{section}
\begin{document}
\title{Notes for Kalman Filter EM algorithms}

\section{Introduction:}

This document serves as the theoretical foundation of the linkalman package. Compared with some other popular kalman filter package written in python, linkalman has several advantages:

\begin{boenumerate}
    \item Account for incomplete measurements 
    \item flexible model structure
    \item Robust numerical implementation
\end{boenumerate}

Baseline kalman filters assume constant parameter matrices such that the EM solver has closed solutions. In practice, the dynamic system may assume complex functional forms and may have incomplete measurements. In addition, solving a kalman filter involves matrix inverse and semi-positiveness of certain matrices, which likely to fail due to rounding errors. linkalman package, while still falls in the linear Gaussian realm, is general enough to tackle many real world problems. This document serves as the theoretical foundation of the linkalman package. Section (\ref{sec:model_setup}) lays out the general structure of a Linear Dynamic System (LDS), and section (\ref{sec:E}) and (\ref{sec:M}) provide detailed derivation of the E-step and the M-step of EM algorithm, respectively.

\section{Model Setup:} \label{sec:model_setup}

Consider the following LDS:

\begin{align}
    \xi_{t+1} = & F_{t}\xi_{t} + B_{t}x_t + v_{t+1} \label{eq:state_evolve} \\
    y_t = & H_t\xi_{t} + D_{t}x_t + w_t \label{eq:measure}
\end{align}

Equation (\ref{eq:state_evolve}) governs the transition process. $\xi_t$ $(m\times 1)$ is the latent state at time $t$, $x_t$ $(k\times 1)$ is the deterministic input signal, $v_t$ $(m\times 1)$ is the exogeneous process noise. We assume that $v_t\sim N(0,Q_t)$ is white noise\footnote{For process noises that are not white noise, we can re-write equation (\ref{eq:state_evolve}) to maintain independence across time.}. $F_t$ $(m\times m)$, $B_t$ $(m\times k)$, and $v_t$ specify the transition behaviors between $t$ and $t+1$. 

Equation (\ref{eq:measure}) is the measurement specification. $y_t$ $(n\times 1)$ is the measurement vector at time $t$. $w_t$ $(n\times 1)$ is the exogeneous measurement noise. In addition to assuming $w_t\sim N(0, R_t)$ is white noise, I aslo assume that  $w_t \perp v_s \ \forall\  t,s\in\{0,1,...,T\}$. $H_t$ $(n\times m)$ and $D_t$ $(n\times k)$ dictate interaction among $\xi_t$, $y_t$ and $x_t$. 

Equations (\ref{eq:state_evolve}) and (\ref{eq:measure}) characterize a Hidden Markov Model (HMM), with system matrices $M_t\equiv\{F_t, B_t, H_t, D_t, Q_t, R_t\}$. The subscript $t$ makes the model very flexible. For example, regression effects in time series models are placed in $B_t x_t$; ARMA can be modeled by $F_t\xi_t$ and $v_t$; additive outliers fit into $B_t x_t$. The subscript $t$ allows the LDS to evolve over time, only constrained by some underlying parameters $\theta$. For example cyclical pattern can be modeled by using Trigonometric Cycle (\cite{harvey_1985}).  

\section{Kalman Filter} \label{sec:filter}

Suppose we observe $Y_T$, and $\theta$ is known. To predict $y_{T+1}$, we need to perform forward filtering Kalman Filtering. We start by making the following notations:

\begin{align*}
    Y_t &\equiv \{y_1, y_2, ..., y_t\} \\
    X_t &\equiv \{x_1, x_2, ..., x_t\} \\
    \hat{\xi}_{t,s} &\equiv E(\xi_t|X_{s},Y_{s};\theta) \\
    \hat{\xi}_{t,r,s} &\equiv E(\xi_t|\xi_r,X_{s},Y_{s};\theta) \\
    P_{t,s} &\equiv E[(\xi_t-\hat{\xi}_{t,s})(\xi_t-\hat{\xi}_{t,s})']
\end{align*}

With the assumption that $w_t$ and $v_t$ are Gaussian white noise, conditional distribution of $y_t$ and $\xi_t$ are fully characterized by Gaussian process with $\hat{\xi}_{t,s}$ and $P_{t,s}$. Now consider at time $t$, we know $Y_t$, $\hat{\xi}_{t,t-1}$, $P_{t,t-1}$ and $M_t$. By equation (\ref{eq:state_evolve}), we have\footnote{The derivation closely follows Chapter 13 by Hamilton (\cite{hamilton_1994}).}:

\begin{align}
    \hat{\xi}_{t,t} &= \hat{\xi}_{t,t-1} + K_t(y_t-H_t\hat{\xi}_{t,t-1}-D_tx_t) \label{eq:filter_begin} \\
    K_t &= P_{t,t-1}H_t^{'}(H_tP_{t,t-1}H_t^{'}+R_t)^{-1} \label{eq:gain} \\
    P_{t,t} &= P_{t,t-1} - K_tH_tP_{t,t-1} \label{eq:p_tt} \\
    \hat{\xi}_{t+1,t} &= F_t\hat{\xi}_{t,t} + B_tx_t \\
    P_{t+1,t} &= F_tP_{t,t}F_t^{'}+Q_{t+1} \label{eq:filter_end}
\end{align}

The Kalman Filter proceeds as follows:

\begin{boenumerate}
    \item Begin with initial value $\hat{\xi}_{1,0}$, and $P_{1,0}$, which can either be part of system parameter $\theta$ or be measured. 
    \item Use equation (\ref{eq:filter_begin}) through (\ref{eq:filter_end}) to calculate $\hat{\xi}_{2,1}$, and $P_{2,1}$.
    \item Repeat step 2 for $t\in\{3, 4, ..., T\}$.
\end{boenumerate}

\subsection{Joseph Form and Numerical Robustness}

We define $P_{t,t}$ as a covariance matrix, which is a symmetric positive semi-definite (PSD) matrix, but in practice, we may get $P_{t,t}$ that is neither symmetric nor PSD, due to rounding errors. Look at equation (\ref{eq:p_tt}) again. The subtraction makes calculating $P_{t,t}$ susceptible to rounding errors, sometimes even resulting in negative semi-definite matrix. I use Joseph Form to numerically enforce symmetric PSD. 

The Joseph form of $P_{t,t}$ is\footnote{The proof in Appendix \ref{ap:joseph}}:

\[
    P_{t,t} = [I - K_tH_t]P_{t,t-1}[I - K_tH_t]' + K_tR_tK_t^{'}    
\]

If we guarantee $P_{t,t-1}$ and $R_t$ to be PSD, then $P_{t,t}$ is PSD by construction. 

\subsection{Missing Measurements and Sequential Filtering}

So far we have assume that we observe full measurement $y_t$ for each $t$. If we do not have full observation, we can instead update the Kalman Filter sequentially, based only on observed measurements. Squential filtering also boosts speed dramatically for large matrix, reducing cost from $\mathcal{O}(n^3)$ to $\mathcal{O}(n)$. First assume that $R_t$ is diagonal, with noise variance $\sigma_{t:1}$ for measurement $i$ at time $t$. Equation (\ref{eq:measure}) then becomes:

\begin{align*}
    y_t &= 
    \begin{pmatrix}
        y_{t:1} \\
        \vdots \\ 
        y_{t:n}
    \end{pmatrix} 
    = \begin{pmatrix}
        H_{t:1}\xi_t + D_{t:1}x_t + w_{t:1} \\
        \vdots \\
        H_{t:n}\xi_t + D_{t:n}x_t + w_{t:n}
    \end{pmatrix}
\end{align*}

where $H_{t:n}$ and $D_{t:n}$ are the $n-th$ row of $H_t$ and $D_t$. Define $\hat{\xi}_{t:i}$ as:
\begin{align*}
    \hat{\xi}_{t,t:i} &\equiv E(\xi_t|X_{t-1},Y_{t-1},x_{t:1},...,x_{t:n},y_{t:1},...,y_{t:n};\theta) \\
    P_{t,t:i} &\equiv Var(\xi_t|X_{t-1},Y_{t-1},x_{t:1},...,x_{t:n},y_{t:1},...,y_{t:n};\theta) 
\end{align*}
We initialize the measurement update process with:
\begin{align}
    \hat{\xi}_{t,t:0} = \hat{\xi}_{t,t-1} \label{eq:seq_init1} \\
    P_{t,t:0} = P_{t,t-1} \label{eq:seq_init2}
\end{align}

For each successive measurement $i$, we have:
\begin{align}
    \hat{\xi}_{t,t:i} &= \hat{\xi}_{t,t:i-1} + K_{t:i}(y_{t:i} - H_{t:i}\hat{\xi}_{t,t:i-1} - D_{t:i}x_{t}) \label{eq:seq_start} \\ 
    K_{t:i} &\equiv \frac{P_{t,t:i-1}H_{t:i}^{'}}{H_{t:i}P_{t,t:i-1}H_{t:i}^{'}+\sigma_{t:i}^{2}} \\
    P_{t,t:i} &= [I - K_{t:i}H_{t:i}]P_{t,t:i-1}[I-K_{t:i}H_{t:i}]^{'} + K_{t:i}\sigma_{t:i}^{2}K_{t:i}^{'} \label{eq:seq_end}
\end{align}

Note that because $H_{t:i}P_{t,t:i-1}H_{t:i}^{'}+\sigma_{t:i}^{2}$ is a scalar, we don't have to perform matrix inverse, and this formular is agnostic to the number of measurements in $y_t$. Sequential filter proceeds as follows:
\begin{boenumerate}
    \item Initialize with (\ref{eq:seq_init1}) and (\ref{eq:seq_init2})
    \item \label{step:seq_update} Update Kalman Filter using equation (\ref{eq:seq_start}) through (\ref{eq:seq_end})
    \item Repeat Step \ref{step:seq_update} for each $y_{t:i}$
\end{boenumerate}

If $R_t$ is not diagonal, we can use LDL Decomposition\footnote{In practice, I perform LDL Decomposition with \texttt{scipy.linalg.ldl}.} to transform the original LDS into one with independent measurement noise. Given that $R_t$ is PSD, we have $R_t =$ 

\section{Kalman Smoother} \label{sec:smoother}

\section{EM Algorithm}

\section{Implementation}

\section{Solve the Kalman filter} \label{sec:kf}

Here I use EM algorithm to solve the Kalman filter\footnote{Alternatively, one can use MCMC, which is widely used in Bayesian statistics. In addition, UKF and EnKF will be explored in the future to account for non-linearity.}. The challenge is that we will have partially missing data. For the remainder of this section I will lay out the algorithm to solve our problem.

\subsection{EM Algorithm Premier:} \label{subsec:premier}
Denote $Y$ as observed measurements, $\Xi$ as hidden states, and $\theta$ as parameters to be estimated. We want to optimize:

\begin{align}
    L(\theta) & = log[P(Y|\theta)] \nonumber \\
    & = log\left[\frac{P(Y,\xi|\theta)}{P(\xi|Y,\theta)}\right] \nonumber \\
    & = log[P(Y,\xi|\theta)] - log[P(\xi|Y,\theta)] \label{eq:general_mle}
\end{align}

Take expectation of equation (\ref{eq:general_mle}) wrt some distribution of $\xi$ with pdf $f(\xi)$ and get

\begin{align}
    L(\theta) = & \int f(\xi)log[P(Y,\xi|\theta)]d\xi \nonumber \\
    & - \int f(\xi)log[P(\xi|Y,\theta)]d\xi \nonumber
\end{align}

To optimize $L(\theta)$ we can iterate through the E steps and M steps to achieve a local maximum. By Jensen's inequality, we have the second term in equation (\ref{eq:general_mle}) maximized when $f(\xi)=P(\xi|Y,\theta)$ (E-step). If we define:

\[
    Q = \int log[P(Y,\xi|\theta)]f(\xi|Y,\theta)d\xi
\]

then $Q = L(\theta)$. For a given $\hat{\theta}$ and $P(\xi|Y, \hat{\theta})$, we find $\theta$ to optimize the first term (M-step). Use the new $\theta$ as $\hat{\theta}$ for the next iteration, and we will reach a local maximum. It is important to note that for a given $\hat{\theta}$, $f(\xi|Y, \hat{\theta})$ is a given quantity and does not change wrt. $\theta$. 

\section{E-step:} \label{sec:E}

The E-steps involves forward updating and backward smoothing. First, I make the following definitions:

\begin{align*}
    \hat{\xi}_{t+1|t} & \equiv \hat{E}(\xi_{t+1}|\mathcal{Y}_t) \\
    \hat{\xi}_{t|t} & \equiv \hat{E}(\xi_{t}|\mathcal{Y}_t) \\
    P_{t+1|t} & \equiv E[(\xi_{t+1} - \hat{\xi}_{t+1|t})(\xi_{t+1} - \hat{\xi}_{t+1|t})']
\end{align*}

where $\mathcal{Y}_t\equiv(y_t,y_{t-1},...,y_1)$. 

We start off by providing initial estimates on $\hat{\xi}_{1|0}$ and $P_{1|0}$. By Kalman Update rule\footnote{See Hamilton's Time Series Analysis for details.}, we have:

\begin{align}
    \hat{\xi}_{t+1|t} & = F\hat{\xi}_{t|t-1} + FK_t(y_t - A - H\hat{\xi}_{t|t-1}) \label{eq:xi_update} \\
    P_{t|t} & = P_{t|t-1} - K_tHP_{t|t-1} \label{eq:p_update} \\
    P_{t+1|t} & = FP_{t|t}F'+Q \label{eq:pt1_update} \\
    % K_t & \equiv P_{t|t-1}H'(HP_{t|t-1}H'+R)^{-1} \label{eq:gain}
\end{align}

Note that $K_t$ in equation (\ref{eq:gain}) is the Kalman Gain Matrix. It incorporates $y_t$ and updates $\hat{\xi}_{t|t}$. By using equation (\ref{eq:xi_update}) to (\ref{eq:pt1_update}) recursively, we are able to calculate $\{\hat{\xi}_{t+1|t}\}$ and $\{P_{t+1|t}\}$ for all $t$. 
Since we have data for $t \in (1,2,...,T)$, we may use Kalman Smoothing to obtain better fit. The updating formula are the following:

\begin{align}
    \hat{\xi}_{t|T} & = \hat{\xi}_{t|t} + J_t(\hat{\xi}_{t+1|tT} - \hat{\xi}_{t+1|t}) \label{eq:xi_smooth} \\
    P_{t|T} & = P_{t|t} + J_{t}(P_{t+1|T} - P_{t+1|t})J_{t}^{'} \label{eq:p_smooth} \\
    J_t & \equiv P_{t|t}F'P_{t+1|t}^{-1} \label{eq:back_smooth}
\end{align}

We use equation (\ref{eq:xi_smooth}) and (\ref{eq:p_smooth}) recursively backwards to get $\{P_{t|T}\}$ and $\{\hat{\xi}_{t|T}\}$. 

In theory, equation (\ref{eq:p_update}) should always yield positive semi-definite matrix. In practice, it is possible that we have non-PSD matrix due to numerical rounding errors. I use Joseph Form to guarantee we have a PSD matrix. Rewrite equation (\ref{eq:p_update}) as:

\begin{align} 
    \hat{\xi}_{t|t} & = P_{t|t-1} - K_tHP_{t|t-1} \nonumber \\
    & = (I-K_tH)P_{t|t-1}(I-K_tH)'+K_tRK_t^{'} \label{eq:joseph}
\end{align}

Equation (\ref{eq:joseph}) guarantees the result is PSD 

\subsection{Sequential Processing and Missing Measurements:} \label{subsec:seq}

In practice, we use sequential updating and smoothing. One obvious reason is improvement in numerical performance. Instead of performing the full matrix inverse, which can be prohibitively expensive if the either observation or state vector is large. But most importantly, by using sequential updating, I am able to update Kalman Filter using only partial information, which is not handled by existing Kalman filter packages. 

To perform Kalman updating sequentially, consider again equation (\ref{eq:measure}):
\[
    y_t = A + H\xi_{t} + w_t 
\]
where $E(w_{t}w_{t}')=R$. We use LDL decomposition method to obtain $L$ such that $R=LDL'$. Rewrite equation (\ref{eq:measure}) as:

\[
    \bar{y}_t = L^{-1}A + L^{-1}H\xi_{t} + L^{-1}w_t
\]

where $\bar{w}_t = L^{-1}w_t$. Note that $\bar{w}_t$ has diagonal variance matrix $D$. For the remainder of this section, we assume that we work with transformed measures (thus omit the "-" notation).

Assume at time $t$ we have $k$ measurements. We initialize the measurement update process with $\hat{\xi}_{t|t}^0=\hat{\xi}_{t|t-1}$, and $P_{t|t}^0=P_{t|t-1}$. Consider the measurement update for the $i$th measurement, $y_t^i$:

\begin{align*}
    \hat{\xi}_{t}^i & = \hat{\xi}_{t}^{i-1} + K_t^i(y_t^i - A - H\hat{\xi}_{t|t}^{i-1}) \\
    P_{t|t}^{i} & = (I - K_t^iH_i)P_{t|t}^{i-1}(I-K_t^iH_i)' + K_t^{i}R_{i,i}(K_t^{i})' \\
    K_t^i & = \frac{P_{t|t}^{i-1}H_i^{'}}{H_{i}P_{t|t}^{i-1}H_{i}^{'}+R_{i,i}}
\end{align*}

If we don't have complete set of measurements at time $t$, we can simply skip the updating from any missing measurement.

\section{M-step:} \label{sec:M}

In this section, I detail steps to solve MLE iteratively. Note that to goal is to find parameters $\theta \equiv {A, H, F, R, Q}$ to minimize:

\begin{align}
    L(\theta,y) = \sum_t^{t=T}log[\mathbb{P}(y_t|\theta)] = \sum_t^{t=T}log\{E[\mathbb{P}(y_t,\xi_t|\theta)]_{\xi_t}\} \label{eq:MLE}
\end{align} 

By Markov Properties, we have: 

\begin{align}
    \sum_t^{t=T}E\{log[\mathbb{P}(y_t,\xi_t)]\}_{\xi_t} &= \sum_t^{t=T}log\{E[\mathbb{P}(\xi_t|\xi_{t-1},\theta)]_{\xi_t}\}
    + \sum_t^{t=T}log\{E[\mathbb{P}(y_t|\xi_t, \theta)]_{\xi_t}\} \nonumber \\
    & = \sum_t^{t=T}log\left[\int\mathbb{P}(y_t|\xi_t,\theta)d\xi_t\right]
    + \sum_t^{t=T}log\left[\int\mathbb{P}(\xi_t|\xi_{t-1},\theta)d\xi_t\right] \nonumber 
\end{align}

If we assume Gaussian Process on the error terms, we have:
\begin{align}
    log[\mathbb{P}(\xi_t|\xi_{t-1},\theta)] = \frac{1}{2}log|Q^{-1}|-\frac{1}{2}(\xi_t-F\xi_{t-1})'Q^{-1}(\xi_t-F\xi_{t-1}) + C \nonumber \\
    = \frac{1}{2}log|Q^{-1}|-\frac{1}{2}\{Trace[(\xi_t-F\xi_{t-1})'Q^{-1}(\xi_t-F\xi_{t-1})]\} + C \nonumber \\
    = \frac{1}{2}log|Q^{-1}|-\frac{1}{2}\{Trace[Q^{-1}(\xi_t-F\xi_{t-1})(\xi_t-F\xi_{t-1})']\} + C \label{eq:mle_xi}
\end{align}

The second equality follows from $x = Trace(x)$ if $x$ is a scalar, and the third equality follows from $Trace(AB) = Trace(BA)$. Similarly, for $\mathbb{P}()y_t|\xi_{t},\theta)$, we have:
\begin{align}
    log[\mathbb{P}(y_t|\xi_{t},\theta)] = \frac{1}{2}log|R^{-1}|-\frac{1}{2}(y_t-A-H\xi_t)'R^{-1}(y_t-A-H\xi_t) + C \nonumber \\
    = \frac{1}{2}log|R^{-1}|-\frac{1}{2}\{Trace[R^{-1}(y_t-A-H\xi_{t})(y_t-A-H\xi_{t})']\} + C \label{eq:mle_y} 
\end{align}

Note that we have incomplete observations, so we use cholesky decomposition. (Replace current formulation) 

Before we take derivatives, I list here some useful formula:

\begin{flalign*}
    &\frac{\partial B'AB}{\partial B} = B'(A'+A) \\
    &\frac{\partial Trace(AB)}{\partial A} = \frac{\partial Trace(BA)}{\partial A} = \frac{\partial Trace(B'A')}{\partial A} = B' \\
    &\frac{\partial log(A)}{\partial A} = A^{-1} \\
    &\frac{\partial det(A)}{\partial A} = det(A)(A^{-1})'
\end{flalign*}

Now we find $\hat{\theta}$ by taking derivations of equation (\ref{eq:MLE}). First let's assume we observe $\{\xi\}_t$, we have:

\begin{align}
    log[\mathbb{P}(\xi,y|\theta)] =& \frac{T-1}{2}log|Q^{-1}|+\frac{T}{2}log|R^{-1}| \nonumber \\
    & - \frac{1}{2}Trace\left[Q^{-1}\sum_1^{T-1}(\xi_t-F\xi_{t-1})'(\xi_t-F\xi_{t-1})\right] \nonumber \\
    & - \frac{1}{2}Trace\left[R^{-1}\sum_1^T(y_t-A-H\xi_{t})(y_t-A-H\xi_{t})'\right] \label{eq:mle_prob}
\end{align}

Take FOC on (\ref{eq:mle_prob}), we have:

\begin{align*}
    \frac{\partial log[\mathbb{P}(\xi,y|\theta)]}{\partial F} &= Q^{-1}\left(\sum_1^T\xi_{t+1}\xi_t^{'} 
        - F\xi_{t+1}\xi_{t+1}'\right) \\
    \frac{\partial log[\mathbb{P}(\xi,y|\theta)]}{\partial H} &= R^{-1}\left(\sum_1^Ty_t\xi_t'
        -A\xi_t-H\xi_t\xi_t^{'}\right) \\
    \frac{\partial log[\mathbb{P}(\xi,y|\theta)]}{\partial A} &= R^{-1}\left(\sum_1^Ty_t-A-H\xi_t\right) \\
    \frac{\partial log[\mathbb{P}(\xi,y|\theta)]}{\partial Q^{-1}} &= \frac{T-1}{2}Q 
        - \frac{1}{2}\left(\sum_1^{T-1}(\xi_t-F\xi_{t-1})'(\xi_t-F\xi_{t-1})\right) \\
    \frac{\partial log[\mathbb{P}(\xi,y|\theta)]}{\partial R^{-1}} &= \frac{T}{2}R 
        - \frac{1}{2}\left(\sum_1^T(y_t-A-H\xi_{t})(y_t-A-H\xi_{t})'\right) 
\end{align*}

Set FOC to 0 and take conditional expectations of $\xi$ with pdf $f(\xi_t|y_1^T)$, we have the update rules for parameters of interest:

\begin{align}
    F &= \left[\sum_1^{T}E(\xi_{t+1}\xi_{t}|y_1^T)'\right] \left[\sum_1^{T}E(\xi_{t+1}\xi_{t+1}|y_1^T)'\right]^{-1} \\
    H &= \left[\sum_1^{T}y_tE(\xi_{t}|y_1^T)' - \frac{1}{T}\left(\sum_1^Ty_t\right)\left(\sum_1^TE(\xi_t|y_1^T)\right)\right] 
    \left[\sum_1^{T}E(\xi_{t}\xi_{t}|y_1^T)'\right]^{-1} \\
    A &= \\
    Q &= \\
    R &= 
\end{align}
\section{Implementation:} \label{sec:implement}
The Implementation is simple (we can use )
implement
use scipy.linalg.dtrtri to solve inverse of triangular matrix.
use cholesky decomposition to make sure inverse is still PSD
potential next step is to use efficient smoothing.
and make a general kalman filter. 
Note we might need to redo structure. As the F need to have eigenvalue less than 1. Need to check whether diff is necessary to get good estimates. This section should be completed last, as the model is still subject to change.
Start with general case, and have a section about linear gaussian implementation.
revise M-step formula, as it does not take into account of changing vec of updates, use cholesky decomposition to get sequential update 

\[
    E(v_{t}v_{\tau}^{'})=\begin{cases}
        Q & t=\tau \\
        0 & otherwise
    \end{cases} 
\]
\printbibliography
\appendix
\section{Derivation of Joseph Form of $P_{t,t}$} \label{ap:joseph}
\begin{align*}
    P_{t,t} &= [I - K_tH_t]P_{t,t-1}[I - K_tH_t]' + K_tR_tK_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + K_tH_tP_{t,t-1}H_t^{'}K_t^{'} + K_tR_tK_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + K_t(H_tP_{t,t-1}H_t^{'} + R_t)K_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1} - P_{t,t-1}H_t^{'}K_t^{'} + P_{t,t-1}H_t^{'}K_t^{'} \\
    &= P_{t,t-1} - K_tH_tP_{t,t-1}
\end{align*}
The fourth equality follows from equation (\ref{eq:gain}). 
\end{document}
