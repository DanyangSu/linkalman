{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example solves the same AR(1) problem, but the purpose here is to demonstration the versatitily of `linkalman`. It has the following three changes:\n",
    "\n",
    "1. three different methods to illustrate all three optimizing techniques\n",
    "2. use customized ft with BaseOpt instead of BaseConstantModel\n",
    "3. use nlopt as an alternative solver to illustrate flexibility of linkalman solver config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AR(1) model\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linkalman\n",
    "import scipy\n",
    "from linkalman.models import BaseOpt as BM\n",
    "from linkalman.core.utils import Constant_M\n",
    "import nlopt\n",
    "%matplotlib inline \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization\n",
    "Instead of relying on `BaseConstantModel`, here I directly define the function `my_ft`. An `ft` should have two required positional arguments: `theta` and `T`. It may also contain keyword arguments. In defining a constant model, I also leverage the `linkalman.core.utils.Constant_M` module. If you have a long time series with mostly constant system dynamic matrices, you can use `Constant_M` to save storage spaces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_ft(theta, T, x_0=0):\n",
    "    \"\"\"\n",
    "    AR(1) model with noise. In general, MLE is biased, so the focus should be \n",
    "    more on prediction fit, less on parameter estimation. The \n",
    "    formula here for Ar(1) is:\n",
    "    y_t = c + Fy_{t-1} + epsilon_{t-1}\n",
    "    \"\"\"\n",
    "    # Define theta\n",
    "    phi_1 = 1 / (np.exp(theta[0])+1)\n",
    "    sigma = np.exp(theta[1]) \n",
    "    sigma_R = np.exp(theta[3]) \n",
    "    # Generate F\n",
    "    F = np.array([[phi_1]])\n",
    "    # Generate Q\n",
    "    Q = np.array([[sigma]]) \n",
    "    # Generate R\n",
    "    R = np.array([[sigma_R]])\n",
    "    # Generate H\n",
    "    H = np.array([[1]])\n",
    "    # Generate B\n",
    "    B = np.array([[theta[2]]])\n",
    "    # Generate D\n",
    "    D = np.array([[0]])\n",
    "    \n",
    "    # Build Mt\n",
    "    Ft = Constant_M(F, T)\n",
    "    Bt = Constant_M(B, T)\n",
    "    Qt = Constant_M(Q, T)\n",
    "    Ht = Constant_M(H, T)\n",
    "    Dt = Constant_M(D, T)\n",
    "    Rt = Constant_M(R, T)\n",
    "    xi_1_0 = theta[2] * x_0 / (1 - phi_1)  # calculate stationary mean, x_0 is already np.ndarray\n",
    "    P_1_0 = np.array([[sigma /(1 - phi_1 * phi_1)]])  # calculate stationary cov\n",
    "    \n",
    "    Mt = {'Ft': Ft,\n",
    "          'Bt': Bt,\n",
    "          'Qt': Qt,\n",
    "          'Ht': Ht,\n",
    "          'Dt': Dt,\n",
    "          'Rt': Rt,\n",
    "          'xi_1_0': xi_1_0,\n",
    "          'P_1_0': P_1_0}\n",
    "    return Mt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_solver(param, obj_func, **kwargs):\n",
    "    \"\"\"\n",
    "    More complex solver function than the simple AR(1) case.\n",
    "    The purpose is to provide an example of flexiblity of\n",
    "    building solvers. Note I also suppress grad in nlopt_obj, \n",
    "    as linkalman uses only non-gradient optimizers\n",
    "    \"\"\"\n",
    "    def nlopt_obj(x, grad, **kwargs):\n",
    "        fval_opt = obj_func(x)\n",
    "        if kwargs.get('verbose', False):\n",
    "            print('fval: {}'.format(fval_opt))\n",
    "        return fval_opt\n",
    "\n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, param.shape[0])\n",
    "    obj = lambda x, grad: nlopt_obj(x, grad, **kwargs)\n",
    "    opt.set_max_objective(obj)\n",
    "    opt.set_xtol_rel(kwargs.get('xtol_rel', opt.get_xtol_rel()))\n",
    "    opt.set_ftol_rel(kwargs.get('ftol_rel', opt.get_ftol_rel()))\n",
    "    theta_opt = opt.optimize(param)\n",
    "    fval_opt = opt.last_optimum_value()\n",
    "    if kwargs.get('verbose_opt', False):\n",
    "        print('fval: {}'.format(fval_opt))\n",
    "        \n",
    "    return theta_opt, fval_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "x = 1\n",
    "model = BM()\n",
    "model.set_f(my_ft, x_0=x * np.ones([1, 1]))\n",
    "model.set_solver(my_solver, xtol_rel=1e-4, verbose=True) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some initial parameters\n",
    "theta = np.array([0, -0.1, 0.1, 1])\n",
    "T = 365\n",
    "train_split_ratio = 0.7\n",
    "forecast_cutoff_ratio = 0.8  \n",
    "\n",
    "# Split train data\n",
    "train_split_t = np.floor(T * train_split_ratio).astype(int)\n",
    "\n",
    "# Generate missing data for forcasting\n",
    "forecast_t = np.floor(T * forecast_cutoff_ratio).astype(int)\n",
    "\n",
    "# If we want AR(1) with non-zero stationary mean, we should proivde a constant \n",
    "x_col = ['const']\n",
    "Xt = pd.DataFrame({x_col[0]: x * np.ones(T)})  # use x to ensure constant model\n",
    "\n",
    "# Build simulated data\n",
    "df, y_col, xi_col = model.simulated_data(input_theta=theta, Xt=Xt)\n",
    "\n",
    "# Store fully visible y for comparison later\n",
    "df['y_0_vis'] = df.y_0.copy()  \n",
    "\n",
    "# Splits models into three groups\n",
    "is_train = df.index < train_split_t\n",
    "is_test = (~is_train) & (df.index < forecast_t)\n",
    "is_forecast = ~(is_train | is_test)\n",
    "\n",
    "# Create a training and test data\n",
    "df_train = df.loc[is_train].copy()\n",
    "\n",
    "# Build two kinds of test data (full data vs. test data only)\n",
    "df_test = df.copy()  \n",
    "\n",
    "# Create an offset\n",
    "df_test.loc[is_forecast, ['y_0']] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLY\n",
    "First, I use numerical methods. It is the preferred methods over EM algorithm, because EM need score function to be effective, which is rather limiting. `linkalman` makes a compromise to gain flexibility in handling missing measurements and customized `ft`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -577.4956255811364\n",
      "fval: -573.8692505684113\n",
      "fval: -577.7483144723885\n",
      "fval: -597.6957969116199\n",
      "fval: -574.4687302800978\n",
      "fval: -581.9720539546967\n",
      "fval: -596.3707246236846\n",
      "fval: -571.6204274874208\n",
      "fval: -581.0345733181947\n",
      "fval: -566.3601180316119\n",
      "fval: -561.5056871047798\n",
      "fval: -560.5884455062071\n",
      "fval: -560.4115693123397\n",
      "fval: -560.3100617806385\n",
      "fval: -559.954261045995\n",
      "fval: -559.3702066752151\n",
      "fval: -558.5636438437797\n",
      "fval: -557.7357582353496\n",
      "fval: -557.1551313832518\n",
      "fval: -556.7487950123475\n",
      "fval: -556.7351261479282\n",
      "fval: -556.7227396577634\n",
      "fval: -556.6961317138232\n",
      "fval: -556.8266151808721\n",
      "fval: -556.7158755145693\n",
      "fval: -556.6344815269573\n",
      "fval: -556.6115520481641\n",
      "fval: -556.597979926823\n",
      "fval: -556.5805644434367\n",
      "fval: -556.5589956811486\n",
      "fval: -556.5545626142995\n",
      "fval: -556.5526255893946\n",
      "fval: -556.6438511576076\n",
      "fval: -556.5555916563309\n",
      "fval: -556.5952720533854\n",
      "fval: -556.5501642275244\n",
      "fval: -556.5494787531394\n",
      "fval: -556.5487044270972\n",
      "fval: -556.5479719258241\n",
      "fval: -556.5468997107449\n",
      "fval: -556.5458065124323\n",
      "fval: -556.5449321795918\n",
      "fval: -556.5445871663438\n",
      "fval: -556.5459600186033\n",
      "fval: -556.544152554487\n",
      "fval: -556.5427222443228\n",
      "fval: -556.5421717986471\n",
      "fval: -556.543145816843\n",
      "fval: -556.5431923783322\n",
      "fval: -556.5420875525563\n",
      "fval: -556.5420208610283\n",
      "fval: -556.5420065427523\n",
      "fval: -556.5419906837191\n",
      "fval: -556.5420024594865\n",
      "fval: -556.5421382590672\n",
      "fval: -556.5416575873926\n",
      "fval: -556.541217179389\n",
      "fval: -556.5407842471157\n",
      "fval: -556.5399418384212\n",
      "fval: -556.5394238572649\n",
      "fval: -556.5391654929535\n",
      "fval: -556.5391783326348\n",
      "fval: -556.5439046959568\n",
      "fval: -556.538680707286\n",
      "fval: -556.5395901202021\n",
      "fval: -556.5459040789872\n",
      "fval: -556.53578196706\n",
      "fval: -556.5345954447185\n",
      "fval: -556.5329410502618\n",
      "fval: -556.5297449123567\n",
      "fval: -556.5259313579053\n",
      "fval: -556.5155173460433\n",
      "fval: -556.5049306074872\n",
      "fval: -556.5208075509878\n",
      "fval: -556.6248314713124\n",
      "fval: -556.4938678095427\n",
      "fval: -556.484857120697\n",
      "fval: -556.4691528222518\n",
      "fval: -556.5230253382384\n",
      "fval: -556.8566321694398\n",
      "fval: -556.510981925041\n",
      "fval: -556.5235010447318\n",
      "fval: -556.4663417009257\n",
      "fval: -556.464798239557\n",
      "fval: -556.4682987976684\n",
      "fval: -556.4836760929801\n",
      "fval: -556.4557301741238\n",
      "fval: -556.4443652578086\n",
      "fval: -556.4326992782492\n",
      "fval: -556.4792118281986\n",
      "fval: -556.547848226959\n",
      "fval: -556.4016666248714\n",
      "fval: -556.3798998618872\n",
      "fval: -556.3680881683607\n",
      "fval: -556.3782880715062\n",
      "fval: -556.5355733688748\n",
      "fval: -556.3310977573585\n",
      "fval: -556.3195896296128\n",
      "fval: -556.3150699963853\n",
      "fval: -556.3191029081539\n",
      "fval: -556.3759612374525\n",
      "fval: -556.312903715478\n",
      "fval: -556.3153054282478\n",
      "fval: -556.3356494565076\n",
      "fval: -556.3114466728125\n",
      "fval: -556.3095549829114\n",
      "fval: -556.3098485818191\n",
      "fval: -556.3411281588121\n",
      "fval: -556.2985160881764\n",
      "fval: -556.2842604681161\n",
      "fval: -556.2964155249965\n",
      "fval: -556.3002002438795\n",
      "fval: -556.2655857329357\n",
      "fval: -556.2695753339024\n",
      "fval: -556.2630102750346\n",
      "fval: -556.260316720087\n",
      "fval: -556.3150481821024\n",
      "fval: -556.275751159334\n",
      "fval: -556.2432097809717\n",
      "fval: -556.2364891842104\n",
      "fval: -556.2556825970835\n",
      "fval: -556.2797448314385\n",
      "fval: -556.2428451744556\n",
      "fval: -556.2295834746271\n",
      "fval: -556.2303081464412\n",
      "fval: -556.2286486532819\n",
      "fval: -556.2285546741372\n",
      "fval: -556.228366285875\n",
      "fval: -556.2273356986113\n",
      "fval: -556.2268488382269\n",
      "fval: -556.2267037416054\n",
      "fval: -556.2265415743163\n",
      "fval: -556.2260735794512\n",
      "fval: -556.2254107764761\n",
      "fval: -556.2246185289883\n",
      "fval: -556.2251055209531\n",
      "fval: -556.2256245613349\n",
      "fval: -556.2244654956946\n",
      "fval: -556.2234956688768\n",
      "fval: -556.2223554269825\n",
      "fval: -556.221878492185\n",
      "fval: -556.2218222255372\n",
      "fval: -556.2213270400638\n",
      "fval: -556.2204124494737\n",
      "fval: -556.2188680600401\n",
      "fval: -556.2178546669835\n",
      "fval: -556.2165835144798\n",
      "fval: -556.2258916788271\n",
      "fval: -556.2241314930086\n",
      "fval: -556.2154184256779\n",
      "fval: -556.2147279536807\n",
      "fval: -556.2153601900103\n",
      "fval: -556.2411006606881\n",
      "fval: -556.2157006119057\n",
      "fval: -556.2129817580499\n",
      "fval: -556.2188321385895\n",
      "fval: -556.2122962045022\n",
      "fval: -556.2133513834012\n",
      "fval: -556.2123551497277\n",
      "fval: -556.212171781812\n",
      "fval: -556.2121252143512\n",
      "fval: -556.2119762823768\n",
      "fval: -556.211763384257\n",
      "fval: -556.2115273168187\n",
      "fval: -556.2111906359213\n",
      "fval: -556.2106902898746\n",
      "fval: -556.2100614594751\n",
      "fval: -556.2101165834125\n",
      "fval: -556.2112060329838\n",
      "fval: -556.2105337580939\n",
      "fval: -556.2092624708139\n",
      "fval: -556.2089060667372\n",
      "fval: -556.2086691521666\n",
      "fval: -556.2085772714977\n",
      "fval: -556.2081925431373\n",
      "fval: -556.2095332743679\n",
      "fval: -556.2088769246395\n",
      "fval: -556.2081764275524\n",
      "fval: -556.2095692528574\n",
      "fval: -556.2078697104783\n",
      "fval: -556.2079423954144\n",
      "fval: -556.2078631843613\n",
      "fval: -556.2078908546814\n",
      "fval: -556.2078554558742\n",
      "fval: -556.2078292190306\n",
      "fval: -556.2077958228001\n",
      "fval: -556.2077667348678\n",
      "fval: -556.2077123607025\n",
      "fval: -556.2076452090251\n",
      "fval: -556.2076600951867\n",
      "fval: -556.2081281387149\n",
      "fval: -556.2074883776339\n",
      "fval: -556.2072712374063\n",
      "fval: -556.2070764545107\n",
      "fval: -556.207007489963\n",
      "fval: -556.2069732037538\n",
      "fval: -556.2069821493363\n",
      "fval: -556.2068707970055\n",
      "fval: -556.2068360319545\n",
      "fval: -556.2068674306394\n",
      "fval: -556.2068409185242\n",
      "fval: -556.2068183700716\n",
      "fval: -556.2067949750303\n",
      "fval: -556.2067816962532\n",
      "fval: -556.2067511489969\n",
      "fval: -556.2067383996712\n",
      "fval: -556.2067289994262\n",
      "fval: -556.2067302893968\n",
      "fval: -556.2068155478406\n",
      "fval: -556.2067258970881\n",
      "fval: -556.2067225119022\n",
      "fval: -556.2067205117288\n",
      "fval: -556.2067185800188\n",
      "fval: -556.2067184669534\n",
      "fval: -556.2067247341406\n",
      "fval: -556.2067185525866\n",
      "fval: -556.2067220484507\n",
      "fval: -556.2067185613912\n",
      "fval: -556.2067195580064\n",
      "fval: -556.2067196812344\n",
      "fval: -556.2067185184957\n",
      "fval: -556.2067185065338\n",
      "fval: -556.2067186999112\n",
      "fval: -556.2067184148135\n",
      "fval: -556.2067184153403\n",
      "fval: -556.2067185033916\n",
      "fval: -556.2067184098755\n",
      "fval: -556.2067184061063\n",
      "fval: -556.2067184116559\n",
      "fval: -556.2067184658873\n",
      "fval: -556.2067184094004\n",
      "fval: -556.2067184182877\n",
      "fval: -556.2067184065695\n",
      "fval: -556.2067184339913\n",
      "fval: -556.2067184055758\n",
      "fval: -556.2067184055245\n",
      "fval: -556.2067184055161\n"
     ]
    }
   ],
   "source": [
    "# Fit data using LLY:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, \n",
    "              method='LLY')\n",
    "theta_LLY = model.theta_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use EM to Cold Start Optimization\n",
    "One may combine EM and LLY method. EM methods, with score functions have very fast convergence at the beginning. Interested readers may design their own solvers to compute the saddle point directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -718.0616510262953\n",
      "fval: -717.7133759737154\n",
      "fval: -727.1158683077156\n",
      "fval: -727.0517019894729\n",
      "fval: -726.9778661994509\n",
      "fval: -726.8973457599078\n",
      "fval: -726.7838870855956\n",
      "fval: -726.8373133725111\n",
      "fval: -727.156361584093\n",
      "fval: -726.7995475850662\n",
      "fval: -726.7993119039363\n",
      "fval: -726.3722693008765\n",
      "fval: -726.0565808864294\n",
      "fval: -725.8286350406881\n",
      "fval: -725.6040538418486\n",
      "fval: -725.0685575431858\n",
      "fval: -724.5443743315608\n",
      "fval: -724.1179071946653\n",
      "fval: -723.7530103914132\n",
      "fval: -723.391365927951\n",
      "fval: -557.2305061899697\n",
      "fval: -557.9511729778741\n",
      "fval: -557.2388866144707\n",
      "fval: -559.8259573658924\n",
      "fval: -596.0879062791444\n",
      "fval: -561.210736438695\n",
      "fval: -557.2621415150737\n",
      "fval: -556.7942025031384\n",
      "fval: -589.8312713241271\n",
      "fval: -557.1333689188539\n",
      "fval: -557.0004460168749\n",
      "fval: -556.7535024275659\n",
      "fval: -556.6815294806427\n",
      "fval: -556.9068577477553\n",
      "fval: -556.802662935267\n",
      "fval: -556.5868864255687\n",
      "fval: -556.5142170091311\n",
      "fval: -556.6133101581855\n",
      "fval: -556.7879772977361\n",
      "fval: -556.541287810638\n",
      "fval: -556.9829001358115\n",
      "fval: -556.4776183682595\n",
      "fval: -556.4308225351339\n",
      "fval: -556.5014146549893\n",
      "fval: -556.4170864471364\n",
      "fval: -556.3987247899677\n",
      "fval: -556.3965651819475\n",
      "fval: -556.3939561399927\n",
      "fval: -556.3946487559422\n",
      "fval: -556.3919499802362\n",
      "fval: -556.3930486638794\n",
      "fval: -556.3926637808124\n",
      "fval: -556.3914972370085\n",
      "fval: -556.3913551465283\n",
      "fval: -556.3920654815788\n",
      "fval: -556.3913590292335\n",
      "fval: -556.3913280225463\n",
      "fval: -556.3913374290527\n",
      "fval: -556.3913376122804\n",
      "fval: -556.391309906206\n",
      "fval: -556.391277997557\n",
      "fval: -556.3912267855659\n",
      "fval: -556.3912040898152\n",
      "fval: -556.391148669954\n",
      "fval: -556.3910401781743\n",
      "fval: -556.3908949047043\n",
      "fval: -556.3906714148359\n",
      "fval: -556.3903688798318\n",
      "fval: -556.3902580653283\n",
      "fval: -556.390330557708\n",
      "fval: -556.390091450216\n",
      "fval: -556.3898217000472\n",
      "fval: -556.3895560847453\n",
      "fval: -556.3893963761299\n",
      "fval: -556.3889450463168\n",
      "fval: -556.3882802452335\n",
      "fval: -556.3864668192322\n",
      "fval: -556.3834811331817\n",
      "fval: -556.377134034547\n",
      "fval: -556.3646846031266\n",
      "fval: -556.3399997164286\n",
      "fval: -556.3249511832659\n",
      "fval: -556.7683977326705\n",
      "fval: -561.1654028614398\n",
      "fval: -558.3421284862811\n",
      "fval: -584.0058664226268\n",
      "fval: -556.6568788592464\n",
      "fval: -556.6342863104778\n",
      "fval: -556.7483449511252\n",
      "fval: -556.9203455674456\n",
      "fval: -556.3423396239863\n",
      "fval: -556.3242526952499\n",
      "fval: -556.3135751262668\n",
      "fval: -556.3302758353734\n",
      "fval: -556.2987148279016\n",
      "fval: -556.2971639958249\n",
      "fval: -556.3477532657662\n",
      "fval: -556.2931730810188\n",
      "fval: -556.291678481661\n",
      "fval: -556.290452966521\n",
      "fval: -556.2951200435149\n",
      "fval: -556.2921895769412\n",
      "fval: -556.28983339964\n",
      "fval: -556.2896954811408\n",
      "fval: -556.2897053220174\n",
      "fval: -556.2917309026917\n",
      "fval: -556.2898194342281\n",
      "fval: -556.2898512385918\n",
      "fval: -556.2896750503073\n",
      "fval: -556.2896557209062\n",
      "fval: -556.2896634787786\n",
      "fval: -556.2897914271114\n",
      "fval: -556.2896412557932\n",
      "fval: -556.2896323145683\n",
      "fval: -556.2896370992397\n",
      "fval: -556.289618089909\n",
      "fval: -556.2895907336516\n",
      "fval: -556.2895457619045\n",
      "fval: -556.2894542487176\n",
      "fval: -556.2892850041301\n",
      "fval: -556.2889815213264\n",
      "fval: -556.2883969313025\n",
      "fval: -556.2879358572826\n",
      "fval: -556.2869756977848\n",
      "fval: -556.2855301228008\n",
      "fval: -556.2828495340432\n",
      "fval: -556.2779186201099\n",
      "fval: -556.2715042384418\n",
      "fval: -556.2775028412874\n",
      "fval: -556.522538803894\n",
      "fval: -556.2549775317908\n",
      "fval: -556.2406308629974\n",
      "fval: -556.2660421541515\n",
      "fval: -568.9188263522734\n",
      "fval: -556.3572400075195\n",
      "fval: -565.4573867237543\n",
      "fval: -556.7120272006025\n",
      "fval: -558.3094169121232\n",
      "fval: -556.3526383062223\n",
      "fval: -556.5915920591606\n",
      "fval: -556.2553870438027\n",
      "fval: -556.2422815681686\n",
      "fval: -556.2466231458029\n",
      "fval: -556.337172181435\n",
      "fval: -556.239999550254\n",
      "fval: -556.2627612406324\n",
      "fval: -556.2367106843432\n",
      "fval: -556.2370244034371\n",
      "fval: -556.2388738505333\n",
      "fval: -556.236658573104\n",
      "fval: -556.2366558547527\n",
      "fval: -556.2366546444746\n",
      "fval: -556.2366732731065\n",
      "fval: -556.2366399312806\n",
      "fval: -556.2366243707376\n",
      "fval: -556.2365996905564\n",
      "fval: -556.2365593215426\n",
      "fval: -556.2365364178702\n",
      "fval: -556.2365262722028\n",
      "fval: -556.2365317339443\n",
      "fval: -556.236506983615\n",
      "fval: -556.2364664392169\n",
      "fval: -556.2364274805418\n",
      "fval: -556.2364095747822\n",
      "fval: -556.2364204209637\n",
      "fval: -556.236396251285\n",
      "fval: -556.2363756424866\n",
      "fval: -556.2363469413714\n",
      "fval: -556.236290560001\n",
      "fval: -556.2361694114568\n",
      "fval: -556.2359610244172\n",
      "fval: -556.23569181112\n",
      "fval: -556.2351232124919\n",
      "fval: -556.2344715926623\n",
      "fval: -556.2334733216481\n",
      "fval: -556.2327145163653\n",
      "fval: -556.232108265359\n",
      "fval: -556.23164720127\n",
      "fval: -556.2296431845473\n",
      "fval: -556.2268547636975\n",
      "fval: -556.2250795375202\n",
      "fval: -556.2216766905087\n",
      "fval: -556.2193996140401\n",
      "fval: -556.2111110157616\n",
      "fval: -556.2088888035453\n",
      "fval: -556.216133169467\n",
      "fval: -556.2327158232478\n",
      "fval: -556.2081098925038\n",
      "fval: -556.2080909657925\n",
      "fval: -556.2080928374721\n",
      "fval: -556.2069466135254\n",
      "fval: -556.2069915071618\n",
      "fval: -556.206946301745\n",
      "fval: -556.2068743837534\n",
      "fval: -556.2068110532954\n",
      "fval: -556.2068103051978\n",
      "fval: -556.2068850177633\n",
      "fval: -556.2068081689711\n",
      "fval: -556.2068560112941\n",
      "fval: -556.2067821212013\n",
      "fval: -556.206751475579\n",
      "fval: -556.2067464299981\n",
      "fval: -556.2067528851713\n",
      "fval: -556.2067493692598\n",
      "fval: -556.2067414100637\n",
      "fval: -556.2067408354479\n",
      "fval: -556.2067936467591\n",
      "fval: -556.2067407574506\n",
      "fval: -556.2067469467721\n",
      "fval: -556.2067415210696\n",
      "fval: -556.2067404597383\n",
      "fval: -556.2067405035199\n",
      "fval: -556.2067403733688\n",
      "fval: -556.2067401463133\n",
      "fval: -556.2067402207551\n",
      "fval: -556.2067461035824\n",
      "fval: -556.2067401608841\n",
      "fval: -556.2067401619129\n",
      "fval: -556.2067401428164\n",
      "fval: -556.2067400855442\n",
      "fval: -556.2067400251941\n",
      "fval: -556.2067400018996\n",
      "fval: -556.206739992106\n",
      "fval: -556.2067399820463\n",
      "fval: -556.2067399690955\n",
      "fval: -556.2067399002681\n",
      "fval: -556.2067397777847\n",
      "fval: -556.2067396701243\n",
      "fval: -556.2067394799858\n",
      "fval: -556.2067391574088\n",
      "fval: -556.2067390163139\n",
      "fval: -556.2067387018635\n",
      "fval: -556.2067384589589\n",
      "fval: -556.2067376590751\n",
      "fval: -556.2067369344734\n",
      "fval: -556.2067357717123\n",
      "fval: -556.206734441404\n",
      "fval: -556.20673250337\n",
      "fval: -556.2067300115921\n",
      "fval: -556.206726677611\n",
      "fval: -556.2067236768272\n",
      "fval: -556.2067219450497\n",
      "fval: -556.2067221135533\n",
      "fval: -556.2089751167915\n",
      "fval: -556.2067226469192\n",
      "fval: -556.2067687788448\n",
      "fval: -556.2067300928447\n",
      "fval: -556.206730807946\n",
      "fval: -556.206721885508\n",
      "fval: -556.2067201063353\n",
      "fval: -556.2067195853476\n",
      "fval: -556.2067192502259\n",
      "fval: -556.2067192641472\n",
      "fval: -556.2067417062619\n",
      "fval: -556.2067192250789\n",
      "fval: -556.2067192179326\n",
      "fval: -556.2067192186646\n",
      "fval: -556.206719304843\n",
      "fval: -556.2067192063041\n",
      "fval: -556.206719198731\n",
      "fval: -556.2067191970345\n",
      "fval: -556.2067204791009\n",
      "fval: -556.2067191747157\n",
      "fval: -556.2067191600664\n",
      "fval: -556.2067191508796\n",
      "fval: -556.2067191253111\n",
      "fval: -556.2067190976004\n",
      "fval: -556.2067190679165\n",
      "fval: -556.2067190161839\n",
      "fval: -556.2067189518446\n",
      "fval: -556.206718920551\n",
      "fval: -556.2067189428045\n",
      "fval: -556.2067683268102\n",
      "fval: -556.2067188529905\n",
      "fval: -556.2068261328149\n",
      "fval: -556.2067186869031\n",
      "fval: -556.2067186847243\n",
      "fval: -556.2068048435802\n",
      "fval: -556.2067187715393\n",
      "fval: -556.2067218976899\n",
      "fval: -556.2067187052764\n",
      "fval: -556.2067194237172\n",
      "fval: -556.2067186894857\n",
      "fval: -556.2067186513224\n",
      "fval: -556.2067186199023\n",
      "fval: -556.2067186269768\n",
      "fval: -556.2067194808938\n",
      "fval: -556.2067186175478\n",
      "fval: -556.2067186482133\n",
      "fval: -556.2067186169526\n",
      "fval: -556.2067186146778\n",
      "fval: -556.2067186109124\n",
      "fval: -556.2067186121204\n",
      "fval: -556.2067186085757\n",
      "fval: -556.2067186078223\n"
     ]
    }
   ],
   "source": [
    "# Fit data using both methods:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.set_solver(my_solver, xtol_rel=1e-3, ftol_rel=1e-3, verbose_opt=True) \n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, method='EM', num_EM_iter=20)\n",
    "model.set_solver(my_solver, xtol_rel=1e-4, verbose=True) \n",
    "model.fit(df_train, model.theta_opt, y_col=y_col, x_col=x_col, method='LLY')\n",
    "theta_mix = model.theta_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EM\n",
    "In order to use EM algorithm numerically, one has to be careful about tolerance. If the tolerence parameters are too small, it's slow to finish one iteration. On the other hand, due to the iterative nature of EM algorithm, if one set the tolerance too large, it will fail to converge later on. Therefore, EM is better for cold starting a optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -784.5256991758029\n",
      "fval: -754.900354482684\n",
      "fval: -739.7340489933079\n",
      "fval: -731.6770682335448\n",
      "fval: -727.0867949687671\n",
      "fval: -724.8417032572006\n",
      "fval: -724.7266406838334\n",
      "fval: -724.3558081253666\n",
      "fval: -723.9990313647636\n",
      "fval: -723.65572771383\n",
      "fval: -723.32531368526\n",
      "fval: -723.007213423163\n",
      "fval: -722.7008648544398\n",
      "fval: -722.4057239521069\n",
      "fval: -722.1212675229888\n",
      "fval: -721.8469948723018\n",
      "fval: -721.5824286274612\n",
      "fval: -721.327114942055\n",
      "fval: -721.0806232512818\n",
      "fval: -720.8425457107776\n",
      "fval: -720.6124964197877\n",
      "fval: -720.3901105051968\n",
      "fval: -720.175043123842\n",
      "fval: -719.9669684255359\n",
      "fval: -719.7656067301962\n",
      "fval: -719.5707311426008\n",
      "fval: -719.3820251900154\n",
      "fval: -719.199217170325\n",
      "fval: -719.0220512875981\n",
      "fval: -718.8502864920604\n",
      "fval: -718.6836954160103\n",
      "fval: -718.5220633978516\n",
      "fval: -718.3651875868393\n",
      "fval: -718.2128761216186\n",
      "fval: -718.0649473761493\n",
      "fval: -717.9212292670591\n",
      "fval: -717.7815586169919\n",
      "fval: -717.6458220754134\n",
      "fval: -718.5589550268119\n",
      "fval: -718.4229738745073\n",
      "fval: -718.2907181159408\n",
      "fval: -718.1620515377717\n",
      "fval: -718.0368442485903\n",
      "fval: -717.9149723254518\n",
      "fval: -717.7963174824123\n",
      "fval: -717.6807667597869\n",
      "fval: -717.568212232856\n",
      "fval: -717.4585507388078\n",
      "fval: -717.351683620731\n",
      "fval: -717.247516487548\n",
      "fval: -717.1459589888228\n",
      "fval: -717.0469246034538\n",
      "fval: -716.9503304413088\n",
      "fval: -716.8560970569383\n",
      "fval: -716.7641482745357\n",
      "fval: -716.6744110233985\n",
      "fval: -716.5868151831696\n",
      "fval: -716.5012934382103\n",
      "fval: -716.4177811404847\n",
      "fval: -716.3362161804005\n",
      "fval: -716.256545559565\n",
      "fval: -716.1785901504021\n",
      "fval: -716.1024154861966\n",
      "fval: -716.0279682444655\n",
      "fval: -715.9551971048425\n",
      "fval: -715.8840526575563\n",
      "fval: -715.8144873168822\n",
      "fval: -715.7464552392512\n",
      "fval: -715.679912245723\n",
      "fval: -715.6148157485582\n",
      "fval: -715.5511246816349\n",
      "fval: -715.4887994344776\n",
      "fval: -715.4278017899846\n",
      "fval: -715.368094859215\n",
      "fval: -715.3096430357532\n",
      "fval: -715.2524119378593\n",
      "fval: -715.196368356526\n",
      "fval: -715.1414802070987\n",
      "fval: -715.0877164832616\n",
      "fval: -715.0350472132483\n",
      "fval: -714.9834434181605\n",
      "fval: -714.9328770722675\n",
      "fval: -714.8833210651862\n",
      "fval: -714.8320963971477\n",
      "fval: -715.1811367687684\n",
      "fval: -715.132664734148\n",
      "fval: -715.0851433054833\n",
      "fval: -715.0385482345904\n",
      "fval: -714.9928560412809\n",
      "fval: -714.9480439840784\n",
      "fval: -714.9040900318378\n",
      "fval: -714.8609728332656\n",
      "fval: -714.818671700416\n",
      "fval: -714.7771665792382\n",
      "fval: -714.7364380262202\n",
      "fval: -714.6964671860392\n",
      "fval: -714.6572357701777\n",
      "fval: -714.6187260364559\n",
      "fval: -714.580920769431\n",
      "fval: -714.5438032616235\n",
      "fval: -714.5073572955306\n",
      "fval: -714.471567126389\n",
      "fval: -714.4364174656478\n",
      "fval: -714.4018934651203\n",
      "fval: -714.3679807017834\n",
      "fval: -714.3346651631905\n",
      "fval: -714.301933233474\n",
      "fval: -714.2697716799039\n",
      "fval: -714.2381676399866\n",
      "fval: -714.2071086090627\n",
      "fval: -714.1765824284025\n",
      "fval: -714.1465772737572\n",
      "fval: -714.1170816443591\n",
      "fval: -714.0880843523403\n",
      "fval: -714.059574512563\n",
      "fval: -714.0315415328257\n",
      "fval: -714.0039751044548\n",
      "fval: -713.9768651932347\n",
      "fval: -713.950202030688\n",
      "fval: -713.923976105672\n",
      "fval: -713.8981781562919\n",
      "fval: -713.872799162104\n",
      "fval: -713.8478303366103\n",
      "fval: -713.8232631200219\n",
      "fval: -713.799089172282\n",
      "fval: -713.775300366345\n",
      "fval: -713.7518887816902\n",
      "fval: -713.7288466980673\n",
      "fval: -713.7061665894638\n",
      "fval: -713.6838411182852\n",
      "fval: -713.6618631297333\n",
      "fval: -713.6402256463864\n",
      "fval: -713.6189218629634\n",
      "fval: -713.5979451412667\n",
      "fval: -713.5772890053021\n",
      "fval: -713.5569471365599\n",
      "fval: -713.5340561075632\n",
      "fval: -713.8043314234335\n",
      "fval: -713.784331729745\n",
      "fval: -713.7646316474354\n",
      "fval: -713.7452253855902\n",
      "fval: -713.7261072907979\n",
      "fval: -713.707271843266\n",
      "fval: -713.6887136530632\n",
      "fval: -713.6704274564848\n",
      "fval: -713.6524081125365\n",
      "fval: -713.6346505995308\n",
      "fval: -713.6171500117957\n",
      "fval: -713.5999015564889\n",
      "fval: -713.5829005505091\n",
      "fval: -713.5661424175116\n",
      "fval: -713.549622685016\n",
      "fval: -713.5333369816019\n"
     ]
    }
   ],
   "source": [
    "# Fit data using EM:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.set_solver(my_solver, xtol_rel=1e-5, ftol_rel=1e-5, verbose_opt=True) \n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, EM_threshold=0.005, method='EM')\n",
    "theta_EM = model.theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions from LLY:\n",
    "df_LLY = model.predict(df_test, theta=theta_LLY)\n",
    "\n",
    "# Make predictions from mixed models:\n",
    "df_mix = model.predict(df_test, theta=theta_mix)\n",
    "\n",
    "# Make predictions from EM:\n",
    "df_EM = model.predict(df_test, theta=theta_EM)\n",
    "\n",
    "# Make predictions using true theta:\n",
    "df_true = model.predict(df_test, theta=theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true': 1.817851207863637, 'LLY': 1.818411159327936, 'EM': 1.8191099429083135, 'mix': 1.818411205491564}\n",
      "{'true': 0.08827873963664018, 'LLY': -0.050676217516590696, 'EM': -0.01241504069078562, 'mix': -0.05067130414756569}\n"
     ]
    }
   ],
   "source": [
    "# Calculate Statistics\n",
    "RMSE = {}\n",
    "RMSE['true'] = np.sqrt((df_true.y_0_filtered - df_true.y_0_vis).var())\n",
    "RMSE['LLY'] = np.sqrt((df_LLY.y_0_filtered - df_LLY.y_0_vis).var())\n",
    "RMSE['EM'] = np.sqrt((df_EM.y_0_filtered - df_EM.y_0_vis).var())\n",
    "RMSE['mix'] = np.sqrt((df_mix.y_0_filtered - df_mix.y_0_vis).var())\n",
    "\n",
    "M_error = {}\n",
    "M_error['true'] = (df_true.y_0_filtered - df_true.y_0_vis).mean()\n",
    "M_error['LLY'] = (df_LLY.y_0_filtered - df_LLY.y_0_vis).mean()\n",
    "M_error['EM'] = (df_EM.y_0_filtered - df_EM.y_0_vis).mean()\n",
    "M_error['mix'] = (df_mix.y_0_filtered - df_mix.y_0_vis).mean()\n",
    "print(RMSE)\n",
    "print(M_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
