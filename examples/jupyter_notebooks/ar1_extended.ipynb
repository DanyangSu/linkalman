{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Same AR(1) model but using three different methods to illustrate all three optimizing techniques\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import linkalman\n",
    "import scipy\n",
    "from linkalman.models import BaseConstantModel as BCM\n",
    "from linkalman.core.utils import simulated_data, ft, df_to_list, list_to_df, \\\n",
    "        gen_PSD, get_ergodic, create_col, clean_matrix\n",
    "from linkalman.core import Filter\n",
    "import nlopt\n",
    "%matplotlib inline \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_f(theta):\n",
    "    \"\"\"\n",
    "    AR(1) model. In general, MLE is biased, so the focus should be \n",
    "    more on prediction fit, less on parameter estimation. The \n",
    "    formula here for Ar(1) is:\n",
    "    y_t = c + Fy_{t-1} + epsilon_{t-1}\n",
    "    \"\"\"\n",
    "    # Define theta\n",
    "    phi_1 = 1 / (np.exp(theta[0])+1)\n",
    "    sigma = np.exp(theta[1]) \n",
    "    sigma_R = np.exp(theta[2])\n",
    "    # Generate F\n",
    "    F = np.array([[phi_1]])\n",
    "    # Generate Q\n",
    "    Q = np.array([[sigma]]) \n",
    "    # Generate R\n",
    "    R = np.array([[sigma_R]])\n",
    "    # Generate H\n",
    "    H = np.array([[1]])\n",
    "    # Generate B\n",
    "    B = np.array([[theta[3]]])\n",
    "    # Collect system matrices\n",
    "    M = {'F': F, 'Q': Q, 'H': H, 'R': R, 'B': B}\n",
    "\n",
    "    return M\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_solver(param, obj_func, **kwargs):\n",
    "    \"\"\"\n",
    "    More complex solver function than the simple AR(1) case.\n",
    "    The purpose is to provide an example of flexiblity of\n",
    "    building solvers. Note I also suppress grad in nlopt_obj, \n",
    "    as linkalman uses only non-gradient optimizers\n",
    "    \"\"\"\n",
    "    def nlopt_obj(x, grad, **kwargs):\n",
    "        fval_opt = obj_func(x)\n",
    "        if kwargs.get('verbose', False):\n",
    "            print('fval: {}'.format(fval_opt))\n",
    "        return fval_opt\n",
    "\n",
    "    opt = nlopt.opt(nlopt.LN_BOBYQA, param.shape[0])\n",
    "    obj = lambda x, grad: nlopt_obj(x, grad, **kwargs)\n",
    "    opt.set_max_objective(obj)\n",
    "    opt.set_xtol_rel(kwargs.get('xtol_rel', opt.get_xtol_rel()))\n",
    "    opt.set_ftol_rel(kwargs.get('ftol_rel', opt.get_ftol_rel()))\n",
    "    theta_opt = opt.optimize(param)\n",
    "    fval_opt = opt.last_optimum_value()\n",
    "    if kwargs.get('verbose_opt', False):\n",
    "        print('fval: {}'.format(fval_opt))\n",
    "        \n",
    "    return theta_opt, fval_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "x = 1\n",
    "model = BCM()\n",
    "model.set_f(my_f, x_0=x * np.ones([1, 1]))\n",
    "model.set_solver(my_solver, xtol_rel=1e-4, verbose=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake data\n",
    "theta = np.array([-1, -0.1, -0.2, 1])\n",
    "T = 300\n",
    "\n",
    "# Split train data\n",
    "cutoff_t = np.floor(T * 0.7).astype(int)\n",
    "\n",
    "# Generate missing data for forcasting\n",
    "offset_t = np.floor(T * 0.9).astype(int)\n",
    "x_col = ['const']\n",
    "Xt = pd.DataFrame({x_col[0]: x * np.ones(T)})\n",
    "\n",
    "df, y_col, xi_col = model.simulated_data(input_theta=theta, Xt=Xt, T=T)\n",
    "\n",
    "# Create a training set\n",
    "df_train = df.iloc[0:cutoff_t].copy()\n",
    "df_test = df.copy()  # Keep the full data for forward prediction\n",
    "\n",
    "# Create an offset\n",
    "df_test['y_0_vis'] = df_test.y_0.copy()  # fully visible y\n",
    "df_test.loc[df.index >= offset_t, ['y_0']] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -907.1845381817362\n",
      "fval: -1016.424034931049\n",
      "fval: -881.7250694361383\n",
      "fval: -764.3123870067957\n",
      "fval: -796.0824235024727\n",
      "fval: -696.8809390016645\n",
      "fval: -932.7581045790464\n",
      "fval: -1137.3134729615222\n",
      "fval: -1031.8390961017265\n",
      "fval: -510.49091555964196\n",
      "fval: -427.17871099537246\n",
      "fval: -445.4836036098398\n",
      "fval: -410.95317306354644\n",
      "fval: -399.24658035121865\n",
      "fval: -411.39789767545983\n",
      "fval: -391.8433655901493\n",
      "fval: -372.29036942997874\n",
      "fval: -356.07292386816306\n",
      "fval: -352.73205792675947\n",
      "fval: -352.1284518422535\n",
      "fval: -349.89026541556177\n",
      "fval: -350.65054493972804\n",
      "fval: -353.413299162927\n",
      "fval: -349.4480068032395\n",
      "fval: -349.30730660556554\n",
      "fval: -349.29882849282836\n",
      "fval: -349.6139814723312\n",
      "fval: -349.87165969568514\n",
      "fval: -349.36451132135255\n",
      "fval: -349.1991153193835\n",
      "fval: -349.22105027558314\n",
      "fval: -349.1041622628345\n",
      "fval: -349.01716840925866\n",
      "fval: -348.9092279066723\n",
      "fval: -348.8727795774758\n",
      "fval: -348.94415918788593\n",
      "fval: -349.061751338814\n",
      "fval: -348.72100543345\n",
      "fval: -348.63989092073757\n",
      "fval: -348.72321004301\n",
      "fval: -348.4228040216782\n",
      "fval: -348.25238771253714\n",
      "fval: -347.9710605999478\n",
      "fval: -347.3695997043357\n",
      "fval: -346.17622702663664\n",
      "fval: -343.1222388359373\n",
      "fval: -337.8751570555549\n",
      "fval: -400.78687720627846\n",
      "fval: -341.28626143030255\n",
      "fval: -334.3189182948926\n",
      "fval: -336.81683080824496\n",
      "fval: -332.7934787180833\n",
      "fval: -336.8604619256834\n",
      "fval: -333.5814859804735\n",
      "fval: -331.90132226730947\n",
      "fval: -331.25045187412485\n",
      "fval: -330.372324269964\n",
      "fval: -329.47692258278863\n",
      "fval: -327.9903905361443\n",
      "fval: -329.7581581510509\n",
      "fval: -328.8942739041976\n",
      "fval: -326.189331516149\n",
      "fval: -325.26622724414307\n",
      "fval: -323.56195389640203\n",
      "fval: -320.3793145619971\n",
      "fval: -319.553791406456\n",
      "fval: -331.8487391396867\n",
      "fval: -320.4762544137168\n",
      "fval: -317.68381502161714\n",
      "fval: -317.25294779817904\n",
      "fval: -317.08064522297536\n",
      "fval: -316.90177008122663\n",
      "fval: -316.90518161142785\n",
      "fval: -317.59953751371984\n",
      "fval: -316.4905357977348\n",
      "fval: -315.72891186110866\n",
      "fval: -315.8352490966925\n",
      "fval: -318.17310919904764\n",
      "fval: -316.99205637384415\n",
      "fval: -315.8409600113753\n",
      "fval: -315.8632482796613\n",
      "fval: -315.7526301594242\n",
      "fval: -315.69917945257197\n",
      "fval: -315.728727947912\n",
      "fval: -315.7159922029501\n",
      "fval: -315.7038338829456\n",
      "fval: -315.69392661175937\n",
      "fval: -315.68663951391363\n",
      "fval: -315.68297693962245\n",
      "fval: -315.6878384636718\n",
      "fval: -315.68092060627805\n",
      "fval: -315.6781398101196\n",
      "fval: -315.6752017668421\n",
      "fval: -315.6745710000083\n",
      "fval: -315.67428143557044\n",
      "fval: -315.6742116729504\n",
      "fval: -315.67483226368074\n",
      "fval: -315.67400392020687\n",
      "fval: -315.67357555631077\n",
      "fval: -315.67359554452486\n",
      "fval: -315.67366802176167\n",
      "fval: -315.6734316797941\n",
      "fval: -315.6733205356431\n",
      "fval: -315.6730252835572\n",
      "fval: -315.67252524522866\n",
      "fval: -315.6718802925714\n",
      "fval: -315.6718640481511\n",
      "fval: -315.6732753597736\n",
      "fval: -315.6712723843227\n",
      "fval: -315.6704866687836\n",
      "fval: -315.66982578753374\n",
      "fval: -315.669822486098\n",
      "fval: -315.6698593044171\n",
      "fval: -315.66943395993354\n",
      "fval: -315.66927266241873\n",
      "fval: -315.6684554267905\n",
      "fval: -315.66811374956814\n",
      "fval: -315.668675474551\n",
      "fval: -315.67330762970306\n",
      "fval: -315.6689429263037\n",
      "fval: -315.66917882491094\n",
      "fval: -315.6679810683026\n",
      "fval: -315.66798231233537\n",
      "fval: -315.66792864211413\n",
      "fval: -315.6678233829608\n",
      "fval: -315.66769212483825\n",
      "fval: -315.66786053411516\n",
      "fval: -315.66749775576454\n",
      "fval: -315.66750076097986\n",
      "fval: -315.6675911248137\n",
      "fval: -315.6671876263267\n",
      "fval: -315.66676454900477\n",
      "fval: -315.66680205921654\n",
      "fval: -315.66760288707025\n",
      "fval: -315.6664059086029\n",
      "fval: -315.66620532564866\n",
      "fval: -315.6661050139296\n",
      "fval: -315.66605194571815\n",
      "fval: -315.66576202593757\n",
      "fval: -315.6654712526274\n",
      "fval: -315.6662010615652\n",
      "fval: -315.6659983764689\n",
      "fval: -315.6648316243114\n",
      "fval: -315.6648833125148\n",
      "fval: -315.66611780509464\n",
      "fval: -315.66491021590633\n",
      "fval: -315.66466994156946\n",
      "fval: -315.664335385483\n",
      "fval: -315.6636820349783\n",
      "fval: -315.66251691200245\n",
      "fval: -315.66031020075525\n",
      "fval: -315.65877039170687\n",
      "fval: -315.6557232607552\n",
      "fval: -315.6516198774284\n",
      "fval: -315.64827365140445\n",
      "fval: -315.6564928670082\n",
      "fval: -315.6576553604233\n",
      "fval: -315.64570816369206\n",
      "fval: -315.64461920048564\n",
      "fval: -315.64544863805105\n",
      "fval: -315.6590363048032\n",
      "fval: -315.64185427183946\n",
      "fval: -315.6392340537858\n",
      "fval: -315.6388409711157\n",
      "fval: -315.64094020808983\n",
      "fval: -315.6406662883056\n",
      "fval: -315.63688046885795\n",
      "fval: -315.6407812678381\n",
      "fval: -315.640526741203\n",
      "fval: -315.6357227050709\n",
      "fval: -315.63494171844354\n",
      "fval: -315.63434929781346\n",
      "fval: -315.63396865717925\n",
      "fval: -315.63442113575076\n",
      "fval: -315.7336110251399\n",
      "fval: -315.6352353237141\n",
      "fval: -315.63987026599057\n",
      "fval: -315.6342771066941\n",
      "fval: -315.6367723537906\n",
      "fval: -315.63273753719994\n",
      "fval: -315.6320385375943\n",
      "fval: -315.6319715501352\n",
      "fval: -315.63226587584546\n",
      "fval: -315.63264869616444\n",
      "fval: -315.6320724197572\n",
      "fval: -315.6310585689073\n",
      "fval: -315.63011190176644\n",
      "fval: -315.62980247269775\n",
      "fval: -315.6298342607076\n",
      "fval: -315.6304043726749\n",
      "fval: -315.62973009277465\n",
      "fval: -315.62973136411637\n",
      "fval: -315.6296604270585\n",
      "fval: -315.6295569460989\n",
      "fval: -315.62944106162394\n",
      "fval: -315.6293774469615\n",
      "fval: -315.6293708224923\n",
      "fval: -315.6293285339533\n",
      "fval: -315.6293282222824\n",
      "fval: -315.6296887142896\n",
      "fval: -315.62931234188585\n",
      "fval: -315.62927643376753\n",
      "fval: -315.62924488109076\n",
      "fval: -315.62929953550616\n",
      "fval: -315.6292182375929\n",
      "fval: -315.6291715352895\n",
      "fval: -315.6291388030795\n",
      "fval: -315.62917062876056\n",
      "fval: -315.6293784144281\n",
      "fval: -315.62916285913934\n",
      "fval: -315.6291137397653\n",
      "fval: -315.62906913746866\n",
      "fval: -315.62904842593474\n",
      "fval: -315.6290176848423\n",
      "fval: -315.6290209425026\n",
      "fval: -315.6291929491687\n",
      "fval: -315.6289900535334\n",
      "fval: -315.6289258608562\n",
      "fval: -315.62885556251194\n",
      "fval: -315.628809705091\n",
      "fval: -315.6288175944225\n",
      "fval: -315.6294465678884\n",
      "fval: -315.6285873277863\n",
      "fval: -315.6284905065952\n",
      "fval: -315.6283908905297\n",
      "fval: -315.6284377937554\n",
      "fval: -315.6330106929341\n",
      "fval: -315.6282909631868\n",
      "fval: -315.6282010800394\n",
      "fval: -315.6282182721286\n",
      "fval: -315.63252040622956\n",
      "fval: -315.62815469165395\n",
      "fval: -315.6281328852394\n",
      "fval: -315.62813539010455\n",
      "fval: -315.629534075208\n",
      "fval: -315.62808212088504\n",
      "fval: -315.62811881763815\n",
      "fval: -315.6280688695782\n",
      "fval: -315.628055939105\n",
      "fval: -315.62805961451187\n",
      "fval: -315.62833846269535\n",
      "fval: -315.6280354510431\n",
      "fval: -315.6280395962048\n",
      "fval: -315.628056193095\n",
      "fval: -315.6280304833852\n",
      "fval: -315.62803039545014\n",
      "fval: -315.628030621758\n",
      "fval: -315.62803421565627\n",
      "fval: -315.6280304600769\n",
      "fval: -315.62803029786147\n",
      "fval: -315.6280301874544\n",
      "fval: -315.6280301251681\n",
      "fval: -315.6280300854104\n",
      "fval: -315.62803009263604\n",
      "fval: -315.6280305797718\n",
      "fval: -315.6280300773109\n",
      "fval: -315.62803013771384\n",
      "fval: -315.62803009200724\n"
     ]
    }
   ],
   "source": [
    "# Fit data using LLY:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, \n",
    "              method='LLY')\n",
    "theta_LLY = model.theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -578.8225692373226\n",
      "fval: -463.9091518628262\n",
      "fval: -407.7729163136786\n",
      "fval: -367.1816050781739\n",
      "fval: -362.76327940879594\n",
      "fval: -357.6295015837937\n",
      "fval: -346.3572055710148\n",
      "fval: -328.18701916856025\n",
      "fval: -309.65799794070915\n",
      "fval: -303.9410125623983\n",
      "fval: -300.6970140107255\n",
      "fval: -298.29188082980124\n",
      "fval: -296.95257233441515\n",
      "fval: -296.1749648837872\n",
      "fval: -295.64820728967663\n",
      "fval: -295.25845845657255\n",
      "fval: -294.94896058777385\n",
      "fval: -294.6898522026315\n",
      "fval: -294.44974127632383\n",
      "fval: -294.13943122374155\n",
      "fval: -316.51826698190627\n",
      "fval: -441.63083101335906\n",
      "fval: -317.89880842285106\n",
      "fval: -322.95306741481926\n",
      "fval: -637.962923022092\n",
      "fval: -403.3051841736423\n",
      "fval: -317.4990956535794\n",
      "fval: -318.2820372665497\n",
      "fval: -645.3964311430625\n",
      "fval: -317.3813180802517\n",
      "fval: -317.9560911383888\n",
      "fval: -317.06227918026906\n",
      "fval: -316.4290319278428\n",
      "fval: -316.3631445036904\n",
      "fval: -316.3734698988078\n",
      "fval: -316.7518440803525\n",
      "fval: -316.3166065823865\n",
      "fval: -316.26252402938195\n",
      "fval: -316.2584848848128\n",
      "fval: -316.29869118343925\n",
      "fval: -316.97335403695195\n",
      "fval: -316.22628217176515\n",
      "fval: -316.1902403651271\n",
      "fval: -316.14838234431494\n",
      "fval: -316.1387851909443\n",
      "fval: -316.13842430771416\n",
      "fval: -316.1083526119807\n",
      "fval: -316.1093177655454\n",
      "fval: -316.15964368731096\n",
      "fval: -316.09406874831296\n",
      "fval: -316.0896900755737\n",
      "fval: -316.0877673893887\n",
      "fval: -316.08605303167724\n",
      "fval: -316.0826844907117\n",
      "fval: -316.0844510372639\n",
      "fval: -316.19877760779866\n",
      "fval: -316.0920961853492\n",
      "fval: -316.0631432513467\n",
      "fval: -316.0673328685973\n",
      "fval: -316.06394729641477\n",
      "fval: -316.0607372026689\n",
      "fval: -316.05614445574423\n",
      "fval: -316.05097188086364\n",
      "fval: -316.04801532096656\n",
      "fval: -316.04827320018285\n",
      "fval: -316.04223628645525\n",
      "fval: -316.03773136042867\n",
      "fval: -316.032190035473\n",
      "fval: -316.02173542764615\n",
      "fval: -316.00396382652787\n",
      "fval: -315.98032688265\n",
      "fval: -315.9612215644651\n",
      "fval: -315.94134528545015\n",
      "fval: -315.931390180605\n",
      "fval: -315.919814173579\n",
      "fval: -315.9142743477374\n",
      "fval: -315.89895638837635\n",
      "fval: -315.89967719866553\n",
      "fval: -315.8836930683219\n",
      "fval: -315.86223238424395\n",
      "fval: -315.84246230461565\n",
      "fval: -315.7900169840548\n",
      "fval: -315.72510496208014\n",
      "fval: -315.6962966967003\n",
      "fval: -315.6953615980129\n",
      "fval: -317.89829905665135\n",
      "fval: -315.6759593396058\n",
      "fval: -315.6890080039336\n",
      "fval: -315.6877547841989\n",
      "fval: -315.6689041815747\n",
      "fval: -315.6704226556407\n",
      "fval: -315.66380181232364\n",
      "fval: -315.6649921721073\n",
      "fval: -315.66297718635025\n",
      "fval: -315.66192325788035\n",
      "fval: -315.6625227492695\n",
      "fval: -315.66611052628065\n",
      "fval: -315.6604146414271\n",
      "fval: -315.659124135087\n",
      "fval: -315.65699785740424\n",
      "fval: -315.6548384046212\n",
      "fval: -315.6518676569658\n",
      "fval: -315.65050304055114\n",
      "fval: -315.6527743083262\n",
      "fval: -315.65312088375646\n",
      "fval: -315.64872801844206\n",
      "fval: -315.6487722032003\n",
      "fval: -315.6608094582442\n",
      "fval: -315.6485056213528\n",
      "fval: -315.64848724303215\n",
      "fval: -315.6484363251879\n",
      "fval: -315.6484040574287\n",
      "fval: -315.64850788481755\n",
      "fval: -315.64842473587544\n",
      "fval: -315.6481905759375\n",
      "fval: -315.6478120716411\n",
      "fval: -315.6474501114339\n",
      "fval: -315.64806652234535\n",
      "fval: -315.6495229548204\n",
      "fval: -315.6471370121727\n",
      "fval: -315.6466321692346\n",
      "fval: -315.6481186796092\n",
      "fval: -315.6469000583065\n",
      "fval: -315.6463326921337\n",
      "fval: -315.6466734714791\n",
      "fval: -315.6462215189127\n",
      "fval: -315.64607513097445\n",
      "fval: -315.6457214728496\n",
      "fval: -315.6450865769863\n",
      "fval: -315.6443332499704\n",
      "fval: -315.6428544329047\n",
      "fval: -315.6409106532284\n",
      "fval: -315.64005509615305\n",
      "fval: -315.6376990955777\n",
      "fval: -315.6347990769632\n",
      "fval: -315.63115484901056\n",
      "fval: -315.6299473631446\n",
      "fval: -315.63002137638057\n",
      "fval: -315.6424817241781\n",
      "fval: -315.6295680856865\n",
      "fval: -315.629189051783\n",
      "fval: -315.6295974959837\n",
      "fval: -315.6301818557558\n",
      "fval: -315.62885699219316\n",
      "fval: -315.6288797597093\n",
      "fval: -315.6289468504882\n",
      "fval: -315.62882388046773\n",
      "fval: -315.6287494339565\n",
      "fval: -315.6286271421536\n",
      "fval: -315.6284773149132\n",
      "fval: -315.62851508268085\n",
      "fval: -315.6282797693816\n",
      "fval: -315.6283483997523\n",
      "fval: -315.63376962556214\n",
      "fval: -315.6281854236554\n",
      "fval: -315.6281244514387\n",
      "fval: -315.6280713243197\n",
      "fval: -315.62806032668016\n",
      "fval: -315.62804945485993\n",
      "fval: -315.6280448275734\n",
      "fval: -315.6280339129423\n",
      "fval: -315.6280314746413\n",
      "fval: -315.6281756366403\n",
      "fval: -315.62803272761363\n",
      "fval: -315.62833410086847\n",
      "fval: -315.62803101822703\n",
      "fval: -315.62803140167046\n",
      "fval: -315.62807562572965\n",
      "fval: -315.6280334676403\n",
      "fval: -315.6280313667639\n",
      "fval: -315.62803057073023\n",
      "fval: -315.62803016682375\n",
      "fval: -315.6280301513914\n",
      "fval: -315.62803017245034\n",
      "fval: -315.6280300998622\n",
      "fval: -315.62803012892317\n",
      "fval: -315.62803116994803\n",
      "fval: -315.6280303437547\n",
      "fval: -315.6280300718885\n",
      "fval: -315.62803151151724\n",
      "fval: -315.6280300648458\n",
      "fval: -315.6280300637736\n",
      "fval: -315.62803044215997\n",
      "fval: -315.628030053282\n",
      "fval: -315.6280300516953\n"
     ]
    }
   ],
   "source": [
    "# Fit data using both methods:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.set_solver(my_solver, xtol_rel=1e-3, ftol_rel=1e-3, verbose_opt=True) \n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, method='EM', num_EM_iter=20)\n",
    "model.set_solver(my_solver, xtol_rel=1e-4, verbose=True) \n",
    "model.fit(df_train, model.theta_opt, y_col=y_col, x_col=x_col, method='LLY')\n",
    "theta_mix = model.theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fval: -707.730638725526\n",
      "fval: -581.195069731019\n",
      "fval: -478.2593948942312\n",
      "fval: -411.94477632830433\n",
      "fval: -376.0808819174674\n",
      "fval: -352.9841773415413\n",
      "fval: -331.41088953869394\n",
      "fval: -317.5800275547007\n",
      "fval: -306.29856659768325\n",
      "fval: -300.1836235198538\n",
      "fval: -296.7152088189341\n",
      "fval: -295.06858387260786\n",
      "fval: -292.9582800608866\n",
      "fval: -291.8763157623267\n",
      "fval: -290.98043678372716\n",
      "fval: -290.26900183267054\n",
      "fval: -289.3624104723424\n",
      "fval: -288.80411852805554\n",
      "fval: -288.03548461966744\n",
      "fval: -287.2894478847038\n",
      "fval: -286.7928936961398\n",
      "fval: -286.3185219313366\n",
      "fval: -286.21175434596114\n",
      "fval: -285.61948219186513\n",
      "fval: -284.7883344139889\n",
      "fval: -284.34028292298933\n",
      "fval: -283.81102962032514\n",
      "fval: -283.7542402075315\n",
      "fval: -283.7376853678546\n",
      "fval: -283.71204290700166\n",
      "fval: -283.6839244767511\n",
      "fval: -283.65311092504265\n",
      "fval: -283.61989354746885\n",
      "fval: -283.18950933249647\n",
      "fval: -282.8730681146071\n",
      "fval: -282.54964573002746\n",
      "fval: -282.42282017020636\n",
      "fval: -282.318194121396\n",
      "fval: -282.2238068453831\n",
      "fval: -282.13302785338163\n",
      "fval: -281.6028250375356\n",
      "fval: -281.5676432585731\n",
      "fval: -281.43962882914195\n",
      "fval: -281.44635439599955\n",
      "fval: -281.46030733267025\n",
      "fval: -281.40173591844643\n",
      "fval: -281.3538977480776\n",
      "fval: -281.35057161332793\n"
     ]
    }
   ],
   "source": [
    "# Fit data using EM:\n",
    "theta_init = np.random.rand(len(theta))\n",
    "model.set_solver(my_solver, xtol_rel=1e-5, ftol_rel=1e-5, verbose_opt=True) \n",
    "model.fit(df_train, theta_init, y_col=y_col, x_col=x_col, EM_threshold=0.005, method='EM')\n",
    "theta_EM = model.theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions from LLY:\n",
    "df_LLY = model.predict(df_test, theta=theta_LLY)\n",
    "\n",
    "# Make predictions from mixed models:\n",
    "df_mix = model.predict(df_test, theta=theta_mix)\n",
    "\n",
    "# Make predictions from EM:\n",
    "df_EM = model.predict(df_test, theta=theta_EM)\n",
    "\n",
    "# Make predictions using true theta:\n",
    "df_true = model.predict(df_test, theta=theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'true': 1.3313145136878208, 'LLY': 1.3253762455024292, 'EM': 1.3259338453361957, 'mix': 1.3253756889816979}\n",
      "{'true': -0.07617562463886439, 'LLY': -0.0292044390201727, 'EM': -0.029088758798228913, 'mix': -0.029202154890296938}\n"
     ]
    }
   ],
   "source": [
    "# Calculate Statistics\n",
    "RMSE = {}\n",
    "RMSE['true'] = np.sqrt((df_true.y_0_filtered - df_true.y_0_vis).var())\n",
    "RMSE['LLY'] = np.sqrt((df_LLY.y_0_filtered - df_LLY.y_0_vis).var())\n",
    "RMSE['EM'] = np.sqrt((df_EM.y_0_filtered - df_EM.y_0_vis).var())\n",
    "RMSE['mix'] = np.sqrt((df_mix.y_0_filtered - df_mix.y_0_vis).var())\n",
    "\n",
    "M_error = {}\n",
    "M_error['true'] = (df_true.y_0_filtered - df_true.y_0_vis).mean()\n",
    "M_error['LLY'] = (df_LLY.y_0_filtered - df_LLY.y_0_vis).mean()\n",
    "M_error['EM'] = (df_EM.y_0_filtered - df_EM.y_0_vis).mean()\n",
    "M_error['mix'] = (df_mix.y_0_filtered - df_mix.y_0_vis).mean()\n",
    "print(RMSE)\n",
    "print(M_error)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
